#!/usr/bin/python3 -u
"""Test Harness for Linux System Roles"""

import argparse
import datetime
import fnmatch
import glob
import html
import json
import logging
import os
import errno
import random
import re
import signal
import shlex
import shutil
import socket
import subprocess
import sys
import tempfile
import time
import calendar
import traceback
import urllib.parse
import urllib.request

import requests
import cachecontrol
import cachecontrol.heuristics

import runqemu

# this is what Ansible uses for version comparison
# use LooseVersion to handle alphas, betas, other pre-release versions
from distutils.version import LooseVersion
from checkmeta import role_supported
from compose2image import composeurl2images

HOSTNAME = socket.gethostname()

COMMENT_CMD_TEST_ALL = "[citest]"
COMMENT_CMD_TEST_PENDING = "[citest pending]"
COMMENT_CMD_TEST_BAD = "[citest bad]"
COMMENT_CMD_TEST_SKIP = "[citest skip]"

KVM_BACKEND = "kvm"
CITOOL_BACKEND = "citool"
CITOOL_COMMAND = "/entrypoint.sh"

# https://www.freedesktop.org/wiki/CommonExtendedAttributes/
URL_XATTR = "user.xdg.origin.url"
DATE_XATTR = "user.dublincore.date"

GITHUB_MAX_PAGE_SIZE = 100


class TransientErrorWaitTime:
    TIME_DEFAULT = 30
    TIME_MAX = 600

    def __init__(self):
        self._value = self.TIME_DEFAULT

    def reset(self):
        self._value = self.TIME_DEFAULT

    def next(self):
        ret = self._value
        self._value = min(self._value * 2, self.TIME_MAX)
        return ret


transient_error_wait_time = TransientErrorWaitTime()


def sighandler_exit(signo, frame):
    logging.info(f"Received {signal.Signals(signo).name}, exiting...")
    sys.exit(0)


def handle_transient_httperrors(error):
    """
    Sleep when a transient error occured.

    Return True if slept, False otherwise
    """

    # Handle transient server-side errors
    if error.response.status_code in (500, 502, 503, 504):
        timeout = transient_error_wait_time.next()
        logging.info(
            ">>> Server returned {} {}, waiting {} s...".format(
                error.response.status_code, error.response.reason, timeout
            )
        )
        time.sleep(timeout)
    # Handle rate limiting
    elif (
        error.response.status_code == 403
        and "X-RateLimit-Remaining" in error.response.headers
        and int(error.response.headers["X-RateLimit-Remaining"]) == 0
    ):
        try:
            now = calendar.timegm(
                time.strptime(
                    error.response.headers["Date"], "%a, %d %b %Y %H:%M:%S %Z"
                )
            )
        except ValueError as e:
            logging.warning(
                'Failed to parse date "{}" from headers: {}'.format(
                    error.response.headers["Date"], e
                )
            )
            now = time.time()

        timeout = max(
            int(error.response.headers["X-RateLimit-Reset"]) - now,
            transient_error_wait_time.next(),
        )

        logging.warning("Rate limiting hit, waiting for %s seconds", int(timeout))
        time.sleep(timeout)
    else:
        # If this is error from GitHub, there should be some explanation in response
        # content. Try to print it.
        try:
            github_msg = error.response.json()["message"]
        except Exception:
            github_msg = ""
        else:
            logging.error("GitHub: %s", github_msg)

        if "abuse detection" in github_msg:
            timeout = transient_error_wait_time.next()
            msg = "We have triggered abuse detection, waiting {} s...".format(timeout)
            logging.warning("%s", msg)
            time.sleep(timeout)
            return True

        return False

    return True


def strtobool(val):
    """Convert a string representation of truth to true (1) or false (0).

    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values
    are 'n', 'no', 'f', 'false', 'off', and '0'.  Raises ValueError if
    'val' is anything else.
    """
    val = val.lower()
    if val in ("y", "yes", "t", "true", "on", "1"):
        return 1
    elif val in ("n", "no", "f", "false", "off", "0"):
        return 0
    else:
        raise ValueError("invalid truth value %r" % (val,))


class Session(requests.Session):
    """
    A small extension for requests.Session that saves typing the host, calls
    raise_for_status() on responses by default, and allows specifying a default
    timeout.
    """

    def __init__(self, host, timeout=10):
        super().__init__()
        self.host = host
        self.timeout = timeout

    def post(self, path, json_, check=True):
        logging.debug("POST %s/%s", self.host, path)
        r = super().post(f"{self.host}/{path}", json=json_, timeout=self.timeout)
        if check:
            r.raise_for_status()
        if r.ok:
            transient_error_wait_time.reset()
        return r

    def get(self, path, check=True):
        logging.debug("GET %s/%s", self.host, path)
        r = super().get(f"{self.host}/{path}", timeout=self.timeout)
        if check:
            r.raise_for_status()
        if r.ok:
            transient_error_wait_time.reset()
        return r


class redirect_output:  # pylint: disable=invalid-name
    """
    A context manager that redirects stdout and stderr to a file.
    """

    def __init__(self, filename, mode="w"):
        self.filename = filename
        self.mode = mode
        self.redirect_file = None
        self.oldout = None
        self.olderr = None

    def __enter__(self):
        self.redirect_file = open(self.filename, self.mode, buffering=1)
        sys.stdout.flush()
        sys.stderr.flush()

        self.oldout = sys.stdout
        self.olderr = sys.stderr

        sys.stdout = self.redirect_file
        sys.stderr = self.redirect_file

        return self.redirect_file

    def __exit__(self, *exception):
        self.redirect_file.close()

        sys.stdout = self.oldout
        sys.stderr = self.olderr


class DontCacheStatuses(cachecontrol.heuristics.BaseHeuristic):
    """
    'Heuristics' for preventing caching of statuses.
    """

    def update_headers(self, response):
        # We need to remove 'max-age' from cache-control
        # TODO: figure out a way to discern responses for statuses query
        return {"cache-control": "private"}

    def warning(self, response):
        pass


def run(*argv, env=None, check=True, cwd=None, return_stdout=False):
    """
    Small wrapper around subprocess.run(), which prints the command to be
    executed and raises an exception by default.
    """
    runenv = dict(os.environ)
    if env:
        envrepr = " ".join(["=".join(e) for e in env.items()]) + " "
    else:
        envrepr = ""

    cwdrepr = "cd {}; ".format(shlex.quote(cwd)) if cwd else ""

    if env:
        runenv.update(env)

    print("+ " + cwdrepr + envrepr + " ".join(shlex.quote(a) for a in argv))
    stdout_val = sys.stdout
    if return_stdout:
        stdout_val = subprocess.PIPE

    result = subprocess.run(
        argv,
        env=runenv,
        check=check,
        stdout=stdout_val,
        stderr=sys.stderr,
        cwd=cwd,
        encoding="utf-8",
    )
    logging.debug(f"run: result of {argv} was {result}")
    return result


def get_metadata_from_file(path, attr_key):
    try:
        mdbytes = os.getxattr(path, attr_key)
    except OSError as e:
        if e.errno == errno.ENODATA:
            return None
        raise
    return os.fsdecode(mdbytes)


def image_source_last_modified_by_file_metadata(path):
    return get_metadata_from_file(path, DATE_XATTR) if os.path.exists(path) else ""


def origurl(path):
    """
    Returns the original URL that a given file was downloaded from.
    """
    return get_metadata_from_file(path, URL_XATTR)


def get_metadata_from_url(url, metadata_key):
    with urllib.request.urlopen(url) as url_response:
        return url_response.getheader(metadata_key)


def fetch_image(url, cache, label):
    """
    Fetches an image from @url into @cache as @label if a file with the
    same name downloaded from the same URL doesn't yet exist. There is
    no need for fancier caching, as image URLs are unique enough.

    Labels are not unique enough, because the URL corresponding to
    the label may get updated. And using a filename derived from URL
    would lead to leftover image files filling up the cache directory,
    as nobody would delete them when the URL changes.

    Returns the full path to the image.
    """

    original_name = os.path.basename(urllib.parse.urlparse(url).path)
    nameroot, suffix = os.path.splitext(original_name)
    image_name = label + suffix
    path = os.path.join(cache, image_name)
    image_last_modified_by_src = get_metadata_from_url(url, "Last-Modified")
    image_last_modified_by_file = image_source_last_modified_by_file_metadata(path)

    if (
        not os.path.exists(path)
        or url != origurl(path)
        or image_last_modified_by_src != image_last_modified_by_file
    ):
        logging.debug(f"Fetch {url}")

        image_tempfile = tempfile.NamedTemporaryFile(dir=cache, delete=False)
        try:
            request = urllib.request.urlopen(url)
            shutil.copyfileobj(request, image_tempfile)
            request.close()
        except Exception:  # pylint: disable=broad-except
            logging.warning(traceback.format_exc())
            os.unlink(image_tempfile.name)
            return None

        os.setxattr(image_tempfile.name, URL_XATTR, os.fsencode(url))
        os.setxattr(
            image_tempfile.name, DATE_XATTR, os.fsencode(image_last_modified_by_src)
        )
        os.rename(image_tempfile.name, path)
        print(f"Pulled latest image for {image_name}")
    else:
        print(f"Using cached image for {image_name}")

    return path


class checkout_repository:  # pylint: disable=invalid-name
    """
    A context manager that shallowly checks out a github repository into a
    temporary directory.
    """

    def __init__(self, owner, repo, refspec):
        self.url = f"https://github.com/{owner}/{repo}"
        self.refspec = refspec
        self.dir = None

    def __enter__(self):
        self.dir = tempfile.TemporaryDirectory()

        run("git", "init", "--quiet", self.dir.name)
        run(
            "git",
            "-C",
            self.dir.name,
            "fetch",
            "--quiet",
            "--depth=1",
            self.url,
            self.refspec,
            env={"GIT_TERMINAL_PROMPT": "0"},
        )
        run("git", "-C", self.dir.name, "checkout", "--quiet", "FETCH_HEAD")

        return self.dir.name

    def __exit__(self, *exception):
        self.dir.cleanup()


class Task:
    """
    A task represents a single unit of work: test a specific pull request of a
    repository against an OS image.
    """

    def __init__(self, owner, repo, pull, head, image):
        self.owner = owner
        self.repo = repo
        self.pull = pull
        self.head = head
        self.image = image

        self.id_ = f"pull-{owner}_{repo}-{pull}-{self.head[:7]}-" + image["name"]
        if self.pull == "0":
            self.refspec = "HEAD"
        else:
            self.refspec = f"pull/{self.pull}/head"

    def is_pr(self):
        return self.pull != "0"

    def __str__(self):
        return f"task {self.owner}/{self.repo}/{self.pull}:{self.image['name']}"

    def get_url(self):
        source = self.image.get("source")
        if source:
            return source
        compose_url = self.image.get("compose")
        if compose_url:
            variant = self.image.get("variant")
            image_urls = composeurl2images(compose_url, "x86_64", variant)
            if len(image_urls) == 1:
                return image_urls[0]
            else:
                if image_urls:
                    logging.error(
                        f"ERROR: Multiple images found: {image_urls}"
                        "in compose {compose_url}"
                    )
                else:
                    logging.error(f"ERROR: no image found in compose {compose_url}")
        else:
            logging.error(
                "ERROR: neither source nor compose specified"
                f"in image {self.image['name']}"
            )

    def is_role_distro_supported(self):
        """
        Return True if image is supported. Otherwise False.
        """
        distro, version = self.image["name"].split("-")[:2]
        if version == "x":
            image_filename = os.path.basename(
                urllib.parse.urlparse(self.get_url()).path
            )
            # We expect the first number in the URL image filename to be the major
            # version
            version = re.search(r"\d+", image_filename).group()

        try:
            with checkout_repository(
                self.owner, self.repo, self.refspec
            ) as repo_tmp_dir:
                is_supported = role_supported(
                    f"{ repo_tmp_dir }/meta/main.yml", distro, version
                )
                if is_supported:
                    logging.debug(
                        f"The role { self.repo } supports platform "
                        f"{ self.image['name'] }."
                    )
                else:
                    logging.debug(
                        f"The role { self.repo } does not support platform "
                        f"{ self.image['name'] }. Skipping."
                    )
                return is_supported
        except Exception:
            logging.warning(
                "WARNING: Cannot open metafile meta/main.yml. Skipping all images."
            )
            return None

    def run(
        self,
        artifactsdir,
        private_artifactsdir,
        args,
    ):
        """
        Runs the task and puts results into @artifactsdir. Puts non-public
        artifacts into @private_artifactsdir.  Returns True if all
        tests succeeded.
        """

        run_status = True
        erase_old_snapshot = True  # erase first time through loop
        with checkout_repository(self.owner, self.repo, self.refspec) as sourcedir:
            # If args.collections is true, converts the role to the collections format.
            collection_dest_path = tempfile.TemporaryDirectory().name
            need_collections_paths = False
            if args.collections:
                os.environ["COLLECTION_SRC_PATH"] = sourcedir
                os.environ["COLLECTION_ROLE"] = self.repo
                os.environ["COLLECTION_DEST_PATH"] = collection_dest_path
                # The tests dir is copied to the temporary dir
                # which is outside of the collections.
                os.environ[
                    "COLLECTION_TESTS_DEST_PATH"
                ] = tempfile.TemporaryDirectory().name
                os.environ[
                    "COLLECTION_SUBROLE_PREFIX"
                ] = f"private_{self.repo}_subrole_"
                # we share the root logger with role2collection - disable logging here
                save_level = logging.getLogger().level
                try:
                    logging.getLogger().setLevel(logging.ERROR)
                    lsr_r2c_module.role2collection()
                finally:
                    logging.getLogger().setLevel(save_level)
                need_collections_paths = True

            # Fedora's standard test invocation spec mandates running all
            # playbooks matching `tests/tests*.yml`, but linux-system-roles
            # used to be tested by running all playbooks `test/test_*.yml`.
            # Support both, but prefer the standard way. Can be removed once
            # all repos are moved over.
            playbookglob = f"{sourcedir}/tests/tests*.yml"
            playbooks = glob.glob(playbookglob)

            if not playbooks:
                playbooks = glob.glob(f"{sourcedir}/test/test_*.yml")

            if not playbooks:
                print(
                    f"No test playbooks found, please add at least one "
                    f"playbook that matches {playbookglob}."
                )
                return None
            if args.only_tests:
                playbooks = [
                    pb for pb in playbooks for pat in args.only_tests if pat.search(pb)
                ]
            if not playbooks:
                print(
                    f"No test playbooks matched the patterns given.  Please specify a "
                    f"pattern that matches one of {playbookglob}."
                )
                return None

            setup_yml = None
            if args.use_snapshot:
                setup_snapshot = os.path.join(sourcedir, "tests", "setup-snapshot.yml")
                if os.path.exists(setup_snapshot):
                    setup_yml = [setup_snapshot]
            _playbook_set = [{"is_collection": False, "playbooks": playbooks}]

            if args.collections:
                tests_path = os.path.join(
                    os.environ["COLLECTION_TESTS_DEST_PATH"], "tests", self.repo
                )
                playbookglob = tests_path + "/tests*.yml"
                coll_playbooks = glob.glob(playbookglob)
                if args.only_tests:
                    coll_playbooks = [
                        pb
                        for pb in coll_playbooks
                        for pat in args.only_tests
                        if pat.search(pb)
                    ]
                _playbook_set.append(
                    {"is_collection": True, "playbooks": coll_playbooks}
                )
                if args.use_snapshot and not setup_yml:
                    setup_snapshot = os.path.join(tests_path, "setup-snapshot.yml")
                    if os.path.exists(setup_snapshot):
                        setup_yml = [setup_snapshot]

            ansible_log = f"{artifactsdir}/ansible.log"
            if args.backend == CITOOL_BACKEND:
                output_log = f"{private_artifactsdir}/citool.log"
            else:
                output_log = ansible_log
            for pset in _playbook_set:
                src_filter_plugin_path = os.path.join(sourcedir, "filter_plugins")
                dest_filter_plugin_path = os.path.join(sourcedir, "hide_filter_plugins")
                if pset["is_collection"]:
                    # somehow ansible is picking up this path and using the filter
                    # plugin from this directory when running the collection tests
                    # so remove this directory for the collection tests
                    if os.path.isdir(src_filter_plugin_path):
                        os.rename(src_filter_plugin_path, dest_filter_plugin_path)

                for playbook in sorted(pset["playbooks"]):
                    print(f"Testing {playbook}...", end="")
                    test_passed = False
                    with redirect_output(output_log, mode="a"):
                        # Use the qcow2 inventory from standard-test-roles, which
                        # boots a transient VM and runs the playbook against that.
                        # Create a fresh instance for each test playbook. However,
                        # we do need to run the setup (if it exists) in the same
                        # invocation of ansible-playbook, so that that it applies
                        # to the same VM as the test playbook.
                        if args.backend == CITOOL_BACKEND:
                            testenv = self.image.get("env", {})
                            if need_collections_paths or pset["is_collection"]:
                                envname = "ANSIBLE_COLLECTIONS_PATHS"
                                testenv[envname] = collection_dest_path
                            setup_file = self.image["setup"]
                            result = run(
                                CITOOL_COMMAND,
                                "-o",
                                f"{private_artifactsdir}/citool-debug.log",
                                "ansible",
                                "rules-engine",
                                "guest-setup",
                                "--playbooks",
                                setup_file,  # revisit this if we ever support citool
                                "guess-environment",
                                "--image-method=force",
                                "--image",
                                self.image["openstack_image"],
                                "--distro-method=force",
                                "--distro",
                                self.image["openstack_image"],
                                "--compose-method=force",
                                "--compose",
                                self.image["openstack_image"],
                                "openstack",
                                "test-scheduler-sti",
                                "--playbook",
                                playbook,
                                "test-scheduler",
                                "test-schedule-runner-sti",
                                "test-schedule-runner",
                                "test-schedule-report",
                                env=testenv,
                                check=False,
                                cwd=os.path.dirname(playbook),
                            )
                            if result.resultcode == 0:
                                test_passed = True
                        elif args.backend == KVM_BACKEND:
                            logging.debug(
                                "Running playbook [%s] directory [%s] "
                                "is present [%s] cwd [%s]",
                                playbook,
                                collection_dest_path,
                                str(os.path.isdir(collection_dest_path)),
                                os.path.dirname(playbook),
                            )
                            ansible_args = [f"--skip-tags={args.skip_tags}"]
                            if args.debug:
                                ansible_args.append("-vv")
                            ansible_args.extend(["--", playbook])
                            runqemu_kwargs = dict(
                                ansible_args=ansible_args,
                                collection_path=collection_dest_path,
                                pretty=False,
                                profile=False,
                                sourcedir=sourcedir,
                                artifacts=artifactsdir,
                                use_snapshot=args.use_snapshot,
                                remove_cloud_init=args.remove_cloud_init,
                                use_yum_cache=args.use_yum_cache,
                                wait_on_qemu=True,
                                setup_yml=setup_yml,
                                erase_old_snapshot=erase_old_snapshot,
                                post_snap_sleep_time=args.post_snap_sleep_time,
                            )
                            if "runqemu_args" in self.image:
                                runqemu_kwargs.update(self.image["runqemu_args"])
                            fmtstr = (
                                "Calling runqemu with image %s cache %s inventory %s "
                                "args %s collection_path %s sourcedir %s artifacts "
                                "%s use_snapshot %s remove_cloud_init %s "
                                "use_yum_cache %s setup_yml %s erase_old_snapshot %s "
                                "post_snap_sleep_time %s"
                            )
                            logging.debug(
                                fmtstr,
                                self.image,
                                args.cache,
                                args.inventory,
                                *runqemu_kwargs.values(),
                            )
                            try:
                                runqemu.runqemu(
                                    self.image,
                                    args.cache,
                                    args.inventory,
                                    **runqemu_kwargs,
                                )
                                test_passed = True
                            except Exception as exc:
                                logging.debug(
                                    "Test %s failed with %s", playbook, str(exc)
                                )
                            erase_old_snapshot = (
                                False  # only erase first time through loop
                            )
                        else:
                            assert False, "unreachable"

                    if args.backend == CITOOL_BACKEND:
                        ptrn = re.compile(
                            f"{playbook}] Ansible logs are in (.+/ansible-output.txt)"
                        )
                        # parse the ansible output file location from citool-debug.log
                        ansible_output = ""
                        with open(
                            f"{private_artifactsdir}/citool-debug.log"
                        ) as citooldbg:
                            for line in citooldbg:
                                mtch = ptrn.search(line)
                                if mtch:
                                    ansible_output = mtch.group(1)
                                    break
                        # ansible_output is relative to /WORKDIR
                        if ansible_output:
                            # filter out garbage in ansible_output
                            with open(f"/WORKDIR/{ansible_output}") as inf:
                                with open(ansible_log, "w") as outf:
                                    docopy = False
                                    for line in inf:
                                        if line.startswith("---v---v---v---v---v---"):
                                            docopy = True
                                        elif line.startswith("---^---^---^---^---^---"):
                                            break
                                        elif docopy:
                                            outf.write(line)
                        else:
                            logging.error(
                                "ERROR: Could not find location of ansible output "
                                "in citool-debug.log"
                            )
                            print("FAILURE")
                            return False

                    if not test_passed:
                        run_status = False
                        with open(ansible_log, "r") as ansible_file:
                            for line in ansible_file:
                                if runqemu.INVENTORY_FAIL_MSG in line:
                                    print("ERROR: Inventory not properly set up")
                                    run_status = None

                        print("FAILURE")
                        logging.debug(f"test failed for {artifactsdir}")
                    else:
                        print("SUCCESS")
                    if run_status is None or (
                        not run_status and not args.run_all_tests
                    ):
                        if need_collections_paths and not args.keep_results:
                            cleanup_collection_tempdirs(remove_script=False)
                        return run_status
                if pset["is_collection"]:
                    if os.path.isdir(dest_filter_plugin_path):
                        os.rename(dest_filter_plugin_path, src_filter_plugin_path)
            if need_collections_paths and not args.keep_results:
                cleanup_collection_tempdirs(remove_script=False)
            if run_status:
                logging.debug(f"test passed for {artifactsdir}")
            else:
                logging.debug(f"test failed for {artifactsdir}")
            return run_status


def pull_ok_to_test(gh, owner, repo, pull, author, head, title):
    """
    Returns True if we trust the pull request enough to run its code. It must
    either come from a collaborator of the repository (a github user with push
    access) or be marked with the "needs-ci" tag by a collaborator.
    """

    # some PRs do not need to be tested - see if
    # COMMENT_CMD_TEST_SKIP is in title
    if COMMENT_CMD_TEST_SKIP in title:
        logging.debug("Skip testing this PR")
        return False

    result = gh.get(f"repos/{owner}/{repo}/collaborators/{author}", check=False)
    if result.status_code == 204:
        logging.debug(
            "PR is ok to test - is a collaborator "
            f"repos/{owner}/{repo}/collaborators/{author}"
        )
        return True

    # Check if a member commented with a command like
    # [citest commit:<commit-hash>]
    # to allow to check this commit
    comments = get_comments(gh, owner, repo, pull)
    allow_command = f"[citest commit:{head}]"
    for comment in comments:
        if comment["author_association"] == "MEMBER":
            if allow_command in comment["body"]:
                logging.debug("PR is ok to test - allowed by member")
                return True

    result = gh.get(f"repos/{owner}/{repo}/issues/{pull}/labels")
    if result.status_code == 200 and "needs-ci" in (x["name"] for x in result.json()):
        logging.debug("PR is ok to test - has needs-ci label")
        return True

    logging.debug("PR is not ok to test")
    return False


def get_statuses(gh, owner, repo, sha, max_num_statuses):
    """
    Fetches all statuses of the given commit in a repository and returns a dict
    mapping context to its most recent status.
    """
    # https://developer.github.com/v3/repos/statuses/#get-the-combined-status-for-a-specific-ref
    data = {}
    url = f"repos/{owner}/{repo}/commits/{sha}/status?per_page={max_num_statuses}"
    while url:
        response = gh.get(url)
        status = response.json()
        for item in status["statuses"]:
            data[item["context"]] = item
        url = response.links.get("next", {}).get("url")
        if url:
            url = url.replace(gh.host + "/", "")
    return data


def get_comments(gh, owner, repo, pullnr):
    """Get comments for a pull request, optionally after a certain time"""
    total_comments = []
    url = (
        f"repos/{owner}/{repo}/issues/{pullnr}/comments?per_page={GITHUB_MAX_PAGE_SIZE}"
    )
    while url:
        response = gh.get(url)
        comments = response.json()
        total_comments.extend(comments)
        url = response.links.get("next", {}).get("url")
        if url:
            url = url.replace(gh.host + "/", "")
    return total_comments


def get_pull_requests(gh, owner, repo):
    """Get comments for a pull request, optionally after a certain time"""
    total_prs = []
    url = f"repos/{owner}/{repo}/pulls?per_page={GITHUB_MAX_PAGE_SIZE}"
    while url:
        response = gh.get(url)
        prs = response.json()
        total_prs.extend(prs)
        url = response.links.get("next", {}).get("url")
        if url:
            url = url.replace(gh.host + "/", "")
    return total_prs


def get_comment_commands(gh, owner, repo, pullnr):
    """Returns dict with last occurence of each command"""
    commands = {
        COMMENT_CMD_TEST_ALL: "",
        COMMENT_CMD_TEST_BAD: "",
        COMMENT_CMD_TEST_PENDING: "",
        COMMENT_CMD_TEST_SKIP: "",
    }
    comments = get_comments(gh, owner, repo, pullnr)

    for comment in comments:
        for command in commands:
            if command in comment["body"] and commands[command] < comment["updated_at"]:
                commands[command] = comment["updated_at"]

    return commands


def get_status_context(variant, image_name, ansible_id):
    if variant:
        suffix = f" ({variant})"
    else:
        suffix = ""
    return f"{image_name}/{ansible_id}{suffix}"


def choose_task(gh, repos, images, ansible_id, args):
    """
    Collect tasks from open pull requests (one task for each image
    and each open pull).

    Return the first found task. The caller needs to provide shuffled
    repos/images to reduce the probability that other instances choose the same
    task.

    Returns None if there's nothing to do.
    """

    for owner, repo in repos:
        pulls = get_pull_requests(gh, owner, repo)
        random.shuffle(pulls)
        for pull in pulls:
            logging.debug("Evaluating PR {}".format(pull["url"]))
            author = pull["user"]["login"]
            head = pull["head"]["sha"]
            number = pull["number"]
            title = pull["title"]

            if not pull_ok_to_test(gh, owner, repo, number, author, head, title):
                continue

            statuses = get_statuses(gh, owner, repo, head, args.max_num_statuses)
            commands = get_comment_commands(gh, owner, repo, number)

            for image in images:
                status_context = get_status_context(
                    args.variant, image["name"], ansible_id
                )
                status = statuses.get(status_context)

                if check_commit_needs_testing(status, commands):
                    task = Task(owner, repo, number, head, image)
                    logging.debug(
                        f"Testing PR {pull['url']} with image {image['name']}"
                    )
                    return task
                else:
                    logging.debug(
                        f"Not testing PR {pull['url']} with image {image['name']}"
                    )
            else:
                logging.debug("No images to use for testing PR {}".format(pull["url"]))
        else:
            logging.debug(f"No pull requests for {owner}/{repo}")


def check_commit_needs_testing(status, commands):
    """Check if commit needs to be checked"""

    # Check it, if there's no status for it yet
    if not status:
        logging.debug("PR will be tested because it has no status")
        return True

    # or the status is pending without a hostname in the description
    if status["state"] == "pending" and not status.get("description"):
        logging.debug(
            "PR will be tested because it is in state 'pending' and has no description"
        )
        return True

    # or a generic re-check was requested:
    if status["updated_at"] < commands[COMMENT_CMD_TEST_ALL]:
        logging.debug("PR will be tested because it was given comment to test all")
        return True

    # or the status is error or failure and a re-check was requested
    if (
        status["state"] in ("failure", "error")
        and status["updated_at"] < commands[COMMENT_CMD_TEST_BAD]
    ):
        logging.debug(
            "PR will be tested because it was given comment to retest failing tests"
        )
        return True

    # or the status is pending and a re-check was requested
    if (
        status["state"] == "pending"
        and status["updated_at"] < commands[COMMENT_CMD_TEST_PENDING]
    ):
        logging.debug(
            "PR will be tested because it was given comment to retest pending tests"
        )
        return True

    logging.debug("PR will not be tested because it does not meet criteria")
    return False


def scp(source, destination, secrets):
    """Wrapper around scp to upload logs"""
    result = run(
        "scp",
        "-o",
        f"IdentityFile {secrets}/id_rsa",
        "-o",
        f"UserKnownHostsFile {secrets}/known_hosts",
        "-rpq",
        source,
        destination,
        check=False,
    )
    return result.returncode == 0


def make_html(source_file):
    """Create simple html file with navigation links from test.log"""
    links = {"index": ".", "ansible log": "ansible.log"}
    html_file = source_file + ".html"

    anchors = ""
    for name, target in links.items():
        a_html = "<a href='{}'>{}</a> ".format(html.escape(target), html.escape(name))
        anchors += a_html

    with open(source_file) as ifile:
        textdata = ifile.read()

    html_code = """<pre>{}</pre>
{}
    """.format(
        html.escape(textdata), anchors
    )

    with open(html_file, "w") as ofile:
        ofile.write(html_code)

    return html_file


def cleanup_collection_tempdirs(remove_script=True):
    for path_env_var in ["COLLECTION_TESTS_DEST_PATH", "COLLECTION_DEST_PATH"]:
        if path_env_var in os.environ and os.path.isdir(os.environ[path_env_var]):
            shutil.rmtree(os.environ[path_env_var])
    if remove_script:
        for pp in sys.path:
            if os.path.isdir(pp) and str.startswith(pp, "/tmp/tmp"):
                shutil.rmtree(pp)


def handle_task(gh, args, config, task, ansible_id):
    """Process a task"""
    title = f"{HOSTNAME}: {task.owner}/{task.repo}: pull #{task.pull} "
    title += f'({task.head[:7]}) on {task.image["name"]}'
    logging.info(">>> " + title)

    abort = False

    start_time = datetime.datetime.utcnow()

    description = HOSTNAME + "@" + str(start_time)
    target_url = None
    state = None

    status_context = get_status_context(args.variant, task.image["name"], ansible_id)

    dry_run = args.dry_run
    if not dry_run and task.is_pr():
        logging.info("Claiming %s", task)
        # When running multiple instances of this script, there's a race
        # between choosing a task and setting the status on GitHub to "pending"
        # (there's no race-free way to only set the status for a context when
        # it doesn't yet exist).  Running the same tests multiple times does
        # not affect the resulting status on GitHub, as test runs should be
        # deterministic. We don't need to be perfect in avoiding it.
        #
        # Sleep for a couple of seconds after setting the task to "pending"
        # with our hostname as description. If the same description is set when
        # we wake up, we know that nobody else wants to do the same task and
        # can go ahead. Otherwise, choose something else.

        gh.post(
            f"repos/{task.owner}/{task.repo}/statuses/{task.head}",
            {"context": status_context, "state": "pending", "description": description},
        )

    try:
        if not dry_run and task.is_pr():
            time.sleep(random.randint(5, 20))

            statuses = get_statuses(
                gh, task.owner, task.repo, task.head, args.max_num_statuses
            )
            status = statuses.get(status_context)
            if status["description"] != description:
                logging.info(
                    "Skip: another instance is working on this task: "
                    f"{status['description']}"
                )
                # avoid overwriting status from another instance
                dry_run = True
                return

        role_supported_state = task.is_role_distro_supported()
        if role_supported_state is False:
            state = "success"
            description = (
                f"The role does not support this platform. Skipping. {description}"
            )
            return
        elif role_supported_state is None:
            state = "failure"
            description = (
                f"The role does not contain meta/main.yml file! Skipping. {description}"
            )
            return

        logging.info("Starting %s", task)
        timestamp = start_time.strftime("%Y%m%d-%H%M%S")

        workdir = tempfile.mkdtemp(prefix=f"linux-system-role-test-work-{task.id_}-")
        private_workdir = tempfile.mkdtemp(
            prefix=f"linux-system-role-test-work-private-{task.id_}-"
        )
        artifactsdir = f"{workdir}/artifacts"
        os.makedirs(artifactsdir)
        private_artifactsdir = f"{private_workdir}/artifacts"
        os.makedirs(private_artifactsdir)

        with redirect_output(f"{artifactsdir}/test.log"):
            print(title)
            print(len(title) * "=")
            print()
            try:
                result = task.run(
                    artifactsdir,
                    private_artifactsdir,
                    args,
                )
                logging.debug(f"task result {result} for {artifactsdir}")
                if result:
                    state = "success"
                elif result is None:
                    state = "error"
                    description += ": Error running tests"
                else:
                    state = "failure"

            # Do not handle these exceptions
            except (KeyboardInterrupt, SystemExit):
                logging.debug(f"task terminated unexpectedly for {artifactsdir}")
                raise

            # pylint: disable=broad-except,invalid-name
            except Exception as e:
                if isinstance(e, OSError):
                    # No space left on device
                    # pylint: disable=no-member
                    if e.errno == 28:
                        abort = True

                print(traceback.format_exc())
                state = "error"
                description += ": Exception when running tests: " + str(e)
                logging.error(description)

        run("chmod", "a+rX", workdir)

        local_test_log = f"{artifactsdir}/test.log"

        if dry_run and task.is_pr():
            logging.info(
                f"Artifacts kept at: {artifactsdir}; "
                f"private artifacts at {private_artifactsdir}"
            )
            with open(local_test_log) as test_log:
                logging.info(test_log.read())
        else:
            make_html(local_test_log)

            if task.image.get("upload_results"):
                results_destination = config["results"]["destination"]
                results_url = config["results"]["public_url"]
                results_dir = f"{task.owner}-{task.repo}-{task.id_}-{timestamp}"

                if scp(workdir, f"{results_destination}/{results_dir}", args.secrets):
                    target_url = f"{results_url}/{results_dir}/artifacts/test.log.html"
                else:
                    logging.error("Could not upload results - check ssh keys")
                    description += ": Error uploading results"
            else:
                # move files to private_artifactsdir
                for ff in os.listdir(artifactsdir):
                    shutil.move(f"{artifactsdir}/{ff}", f"{private_artifactsdir}/{ff}")

            # FIXME: workdir might be kept when python crashes
            if not args.keep_results:
                shutil.rmtree(workdir)
                shutil.rmtree(private_workdir)

    finally:
        duration = (datetime.datetime.utcnow() - start_time).seconds
        logging.info(
            "Finished in %d seconds %s: %s - %s",
            duration,
            task,
            state if state and state != "pending" else "abandoned",
            description,
        )
        while not dry_run and task.is_pr():
            try:
                gh.post(
                    f"repos/{task.owner}/{task.repo}/statuses/{task.head}",
                    {
                        "context": status_context,
                        "state": state or "pending",
                        "target_url": target_url,
                        "description": description
                        if state and state != "pending"
                        else "",
                    },
                )
                break
            except requests.exceptions.HTTPError as err:
                if not handle_transient_httperrors(err):
                    raise
        if not dry_run:
            ratelimit = gh.get("rate_limit").json()
            if ratelimit and isinstance(ratelimit, dict) and "rate" in ratelimit:
                logging.info(
                    "GitHub ratelimit info - remaining %(remaining)s used %(used)s"
                    " limit %(limit)s reset %(reset)s",
                    ratelimit["rate"],
                )
            else:
                logging.info("Unable to get ratelimit info")

    print()

    if abort:
        logging.critical("Fatal exception occurred, aborting...")
        sys.exit(1)


def check_environment(args):
    """
    Check whether the environment is sane.
    Intent here is to fail early for example if /dev/kvm is not available.
    """
    if args.backend == KVM_BACKEND:
        if not os.access("/dev/kvm", os.R_OK | os.W_OK):
            logging.critical("test-harness needs access to /dev/kvm, aborting")
            sys.exit(1)

    elif args.backend == CITOOL_BACKEND:
        try:
            run(CITOOL_COMMAND, "-V", "-o", "/tmp/citool-debug.log")
            run("ansible", "--version")
        except Exception:
            logging.critical(
                f"test-harness needs installed citool command {CITOOL_COMMAND}, "
                "aborting"
            )
            sys.exit(1)


def setup_logging(config_logging, args):
    log_cfg_default = {
        "format": "%(asctime)s: %(levelname)s: %(message)s",
        "datefmt": "%Y-%m-%dT%H:%M:%S%z",
    }
    log_cfg = log_cfg_default.copy()
    log_cfg.update(
        (k, v)
        for k, v in config_logging.items()
        if k in {"format", "style", "datefmt", "level"}
    )
    if args.debug:
        log_cfg["level"] = logging.DEBUG

    initial_log_message = ""
    try:
        # Add stderr handler
        stream_handler = logging.StreamHandler()
        if args.debug:
            level = logging.DEBUG
        else:
            level = config_logging.get("stderr_level", logging.INFO)
        stream_handler.setLevel(level)
        handlers = [stream_handler]

        if "filename" in config_logging:
            # as config may be shared between multiple hosts with shared storage,
            # we may need hostname in log name
            filename = config_logging["filename"].replace("HOSTNAME", HOSTNAME)
            try:
                file_handler = logging.FileHandler(filename)
                if args.debug:
                    level = logging.DEBUG
                else:
                    level = config_logging.get("file_level", logging.NOTSET)
                file_handler.setLevel(level)
                handlers.append(file_handler)
            except PermissionError:
                # running as non-root user, but trying to write to /var/log, usually -
                # log the error, skip logging to file
                initial_log_message = (
                    f"could not open {filename} as uid "
                    f"{os.geteuid()}: PermissionError: "
                    "will not log to file"
                )

        logging.basicConfig(handlers=handlers, **log_cfg)
        if initial_log_message:
            logging.warning(initial_log_message)
    except ValueError:
        logging.basicConfig(**log_cfg_default)
        logging.warning("Invalid logging config, using default", exc_info=True)


def get_ansible_version():
    """determine the version of Ansible"""
    rs = run("ansible", "--version", return_stdout=True)
    match = re.match(r"^ansible \[core (\d+[.]\d+[.]\d+).*", rs.stdout)
    if match and match.groups() and match.group(1):
        return match.group(1)
    match = re.match(r"^ansible (\d+[.]\d+[.]\d+).*", rs.stdout)
    if match and match.groups() and match.group(1):
        return match.group(1)
    raise Exception(f"Could not determine ansible version from {rs.stdout}")


def main():
    signal.signal(signal.SIGTERM, sighandler_exit)
    print("Starting at {}".format(time.asctime()))

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--secrets",
        default=os.environ.get("TEST_HARNESS_SECRETS", "/secrets"),
        help="Directory with secrets",
    )
    parser.add_argument(
        "--config",
        default=os.environ.get("TEST_HARNESS_CONFIG", "/config"),
        help="Directory with config.json",
    )
    parser.add_argument(
        "--cache",
        default=os.environ.get("TEST_HARNESS_CACHE", "/cache"),
        help="Directory for caching VM images",
    )
    parser.add_argument(
        "--inventory",
        default=os.environ.get(
            "TEST_HARNESS_INVENTORY",
            "/usr/share/ansible/inventory/standard-inventory-qcow2",
        ),
        help="Inventory to use for VMs",
    )
    default_pull_request = os.environ.get("TEST_HARNESS_PULL_REQUEST", None)
    if default_pull_request:
        default_pull_request = default_pull_request.split(",")
    parser.add_argument(
        "pull_request",
        nargs="*",
        default=default_pull_request,
        help="Pull requests to test. Example: " "linux-system-roles/network/1",
    )
    parser.add_argument(
        "--use-images",
        default=os.environ.get("TEST_HARNESS_USE_IMAGES", "*"),
        help=(
            "Test pull request only against images matching these patterns.  "
            "This is a comma delimited list of fnmatch patterns which will be "
            "applied to each image name and source.  If any of the patterns "
            "match the name or source, the image will be included in testing."
        ),
    )
    parser.add_argument(
        "--skip-images",
        default=os.environ.get("TEST_HARNESS_SKIP_IMAGES"),
        help=(
            "Do not test pull request against images matching these patterns.  "
            "This is a comma delimited list of fnmatch patterns which will be "
            "applied to each image name and source.  If any of the patterns "
            "match the name or source, the image will be skipped in testing."
        ),
    )
    parser.add_argument(
        "--dry-run",
        default=bool(strtobool(os.environ.get("TEST_HARNESS_DRY_RUN", "False"))),
        action="store_true",
        help="Do not update pull request status or upload artifacts",
    )
    parser.add_argument(
        "--keep-results",
        default=bool(strtobool(os.environ.get("TEST_HARNESS_KEEP_RESULTS", "False"))),
        action="store_true",
        help="For debugging - do not remove the temp results directory",
    )
    parser.add_argument(
        "--variant",
        default=os.environ.get("TEST_HARNESS_VARIANT", ""),
        help="Use this variant for PR test status e.g. staging",
    )
    parser.add_argument(
        "--repositories",
        default=os.environ.get("TEST_HARNESS_REPOSITORIES", None),
        help="Comma delimited list of repositories to override config",
    )
    parser.add_argument(
        "--max-num-statuses",
        type=int,
        default=int(
            os.environ.get("TEST_HARNESS_MAX_NUM_STATUSES", str(GITHUB_MAX_PAGE_SIZE))
        ),
        help=(
            f"Number of statuses to retrieve - default {GITHUB_MAX_PAGE_SIZE} - "
            "github default is 30"
        ),
    )
    parser.add_argument(
        "--backend",
        default=os.environ.get("TEST_HARNESS_BACKEND", KVM_BACKEND),
        choices=[KVM_BACKEND, CITOOL_BACKEND],
        help="Backend for testing",
    )
    parser.add_argument(
        "--collections",
        default=bool(strtobool(os.environ.get("TEST_HARNESS_COLLECTIONS", "False"))),
        action="store_true",
        help="Run CI tests in the roles and collections formats",
    )
    parser.add_argument(
        "--skip-tags",
        default=os.environ.get("TEST_HARNESS_SKIP_TAGS", "tests::cleanup"),
        help="ansible-playbook --skip-tags value",
    )
    parser.add_argument(
        "--debug",
        default=bool(strtobool(os.environ.get("TEST_HARNESS_DEBUG", "False"))),
        action="store_true",
        help="Enable debugging",
    )
    parser.add_argument(
        "--run-all-tests",
        default=bool(strtobool(os.environ.get("TEST_HARNESS_RUN_ALL_TESTS", "False"))),
        action="store_true",
        help=(
            "By default, fail fast - stop testing after first failure.  Use this "
            "parameter to run all tests, and report failure at the end."
        ),
    )
    parser.add_argument(
        "--only-tests",
        default=os.environ.get("TEST_HARNESS_ONLY_TESTS"),
        help=("Only run test playbooks that match these space separated patterns."),
    )
    parser.add_argument(
        "--remove-cloud-init",
        default=bool(
            strtobool(os.environ.get("TEST_HARNESS_REMOVE_CLOUD_INIT", "False"))
        ),
        action="store_true",
        help=("Remove cloud-init from the image first."),
    )
    parser.add_argument(
        "--use-yum-cache",
        default=bool(strtobool(os.environ.get("TEST_HARNESS_USE_YUM_CACHE", "False"))),
        action="store_true",
        help=("Use a YUM/DNF RPM package cache."),
    )
    parser.add_argument(
        "--use-snapshot",
        default=bool(strtobool(os.environ.get("TEST_HARNESS_USE_SNAPSHOT", "False"))),
        action="store_true",
        help=("Prepare and use an image snapshot."),
    )
    parser.add_argument(
        "--post-snap-sleep-time",
        type=int,
        default=int(os.environ.get("TEST_HARNESS_POST_SNAP_SLEEP_TIME", "30")),
        help=("Time to sleep post snap creation."),
    )

    args = parser.parse_args()

    # default values for config
    config = {"repositories": [], "images": []}

    with open(args.config + "/config.json") as configfile:
        config.update(json.load(configfile))
        images = config["images"]
        random.shuffle(images)
        repos = [r.split("/") for r in config.get("repositories", [])]
        random.shuffle(repos)

    # this is used in PR status - context name
    if not args.variant:
        if "variant" not in config:
            # legacy support for old configs that still use "name"
            if config.get("name") == "linux-system-roles-test-staging":
                args.variant = "staging"
        else:
            args.variant = config["variant"]

    if args.repositories:
        repos = [r.split("/") for r in args.repositories.split(",")]
    if args.collections:
        from importlib import import_module

        global lsr_r2c_module
        lsr_r2c_module = import_module("lsr_role2collection")
    if args.only_tests:
        args.only_tests = [re.compile(pat) for pat in args.only_tests.split(" ")]
    setup_logging(config.get("logging", {}), args)

    check_environment(args)

    ansible_version = LooseVersion(get_ansible_version())
    logging.info("Using Ansible version {}".format(ansible_version))
    # this is used in PR status - context name
    ansible_id = f"ansible-{ansible_version.version[0]}.{ansible_version.version[1]}"

    image_patterns = args.use_images.split(",")
    if args.skip_images:
        skip_image_patterns = args.skip_images.split(",")
    else:
        skip_image_patterns = []
    # copy all images to test_images . . .
    test_images = dict([(image["name"], image) for image in images])
    # . . . then remove the ones that do not match the criteria
    for image in images:
        min_ansible_version = LooseVersion(image.get("min_ansible_version", "0"))
        if ansible_version < min_ansible_version:
            if image["name"] in test_images:
                logging.debug(
                    f"Image {image['name']} will not be used for testing because "
                    f"the minimum Ansible version {min_ansible_version} required "
                    f"by the image is greater than the version {ansible_version} "
                    "of Ansible used by the test."
                )
                del test_images[image["name"]]
            continue

        if not image.get("openstack_image") and args.backend == CITOOL_BACKEND:
            logging.debug(
                f"Image {image['name']} will not be used for testing because it is not "
                "supported in Citool backend."
            )
            del test_images[image["name"]]
            continue

        keep_image = False
        for pattern in image_patterns:
            if fnmatch.fnmatch(image["name"], pattern) or fnmatch.fnmatch(
                image.get("source", ""), pattern
            ):
                keep_image = True
                break
        for pattern in skip_image_patterns:
            if fnmatch.fnmatch(image["name"], pattern) or fnmatch.fnmatch(
                image.get("source", ""), pattern
            ):
                keep_image = False
                break
        if not keep_image:
            if image["name"] in test_images:
                logging.debug(
                    f"Image {image['name']} will not be used for testing "
                    "because neither the name nor the source match any "
                    f"of the image patterns in {image_patterns}, or because "
                    "the image matched one of the skip patterns in "
                    f"{skip_image_patterns}"
                )
                del test_images[image["name"]]

    logging.info(
        f"Will test with the following image names: {list(test_images.keys())}"
    )
    test_images = list(test_images.values())
    if args.dry_run:
        token = ""
    else:
        with open(args.secrets + "/github-token") as tokenfile:
            token = tokenfile.read().strip()

    gh = Session("https://api.github.com")
    gh.headers.update(
        {
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "linux-system-roles/test",
        }
    )

    if token:
        gh.headers.update({"Authorization": f"token {token}"})

    # Use CacheControl.
    # Requests on keep-alive connections fail when the remote has closed a
    # connection before we've noticed and sent another request. Thus,
    # always retry sending requests once.
    gh.mount(
        "https://",
        cachecontrol.CacheControlAdapter(max_retries=1, heuristic=DontCacheStatuses()),
    )

    printed_waiting = False

    # special mode - test all roles
    if args.pull_request == ["ALL"]:
        args.pull_request = []
        for owner, repo in repos:
            args.pull_request.append(f"{owner}/{repo}/0")

    logging.info(f"running all tests {args.run_all_tests}")
    for pull_request in args.pull_request:
        logging.debug(f"Processing command line pull request {pull_request}")
        # supports to specify the PR with full URLs or just the path
        parsed_url = urllib.parse.urlparse(pull_request)
        owner, repo, pullnr = (
            parsed_url.path.strip("/").replace("/pull/", "/").split("/")
        )

        if pullnr != "0":
            head = parsed_url.fragment
            pull = gh.get(f"repos/{owner}/{repo}/pulls/{pullnr}").json()

            if not head:
                head = pull["head"]["sha"]
            number = pull["number"]
        else:
            head = "HEAD"
            number = "0"

        for image in test_images:
            task = Task(owner, repo, number, head, image)
            handle_task(
                gh,
                args,
                config,
                task,
                ansible_id,
            )

    # Random delay at startup to prevent triggering abuse detection on GitHub
    # when multiple instances are started in same time
    if not args.dry_run:
        time.sleep(random.randint(0, 60))

    while not args.pull_request:
        try:
            task = choose_task(gh, repos, test_images, ansible_id, args)
            if not task:
                if not printed_waiting:
                    logging.info(">>> No tasks. Waiting.")
                    printed_waiting = True
                else:
                    logging.debug(">>> Still no tasks. Waiting.")
                time.sleep(600)
                continue

            handle_task(
                gh,
                args,
                config,
                task,
                ansible_id,
            )
        except requests.exceptions.HTTPError as err:
            if not handle_transient_httperrors(err):
                raise

        # At this point, we have (probably) printed other messages, so
        # reset printed_waiting
        printed_waiting = False

    if args.collections and not args.keep_results:
        cleanup_collection_tempdirs(remove_script=True)


if __name__ == "__main__":
    sys.exit(main())
