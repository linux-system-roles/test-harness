#!/usr/bin/python3 -u
"""Test Harness for Linux System Roles"""

import argparse
import datetime
import fnmatch
import glob
import html
import json
import logging
import os
import random
import signal
import shlex
import shutil
import socket
import subprocess
import sys
import tempfile
import time
import calendar
import traceback
import urllib.parse
import urllib.request

import requests
import cachecontrol
import cachecontrol.heuristics
import yaml

# this is what Ansible uses for version comparison
# use LooseVersion to handle alphas, betas, other pre-release versions
from distutils.version import LooseVersion
from distutils.util import strtobool
from github import Github

HOSTNAME = socket.gethostname()

COMMENT_CMD_TEST_ALL = "[citest]"
COMMENT_CMD_TEST_PENDING = "[citest pending]"
COMMENT_CMD_TEST_BAD = "[citest bad]"


class TransientErrorWaitTime:
    TIME_DEFAULT = 30
    TIME_MAX = 600

    def __init__(self):
        self._value = self.TIME_DEFAULT

    def reset(self):
        self._value = self.TIME_DEFAULT

    def next(self):
        ret = self._value
        self._value = min(self._value * 2, self.TIME_MAX)
        return ret


transient_error_wait_time = TransientErrorWaitTime()


def sighandler_exit(signo, frame):
    logging.info(f"Received {signal.Signals(signo).name}, exiting...")
    sys.exit(0)


def handle_transient_httperrors(error):
    """
    Sleep when a transient error occured.

    Return True if slept, False otherwise
    """

    # Handle transient server-side errors
    if error.response.status_code in (500, 502, 503, 504):
        timeout = transient_error_wait_time.next()
        logging.info(
            ">>> Server returned {} {}, waiting {} s...".format(
                error.response.status_code, error.response.reason, timeout
            )
        )
        time.sleep(timeout)
    # Handle rate limiting
    elif (
        error.response.status_code == 403
        and "X-RateLimit-Remaining" in error.response.headers
        and int(error.response.headers["X-RateLimit-Remaining"]) == 0
    ):
        try:
            now = calendar.timegm(
                time.strptime(
                    error.response.headers["Date"], "%a, %d %b %Y %H:%M:%S %Z"
                )
            )
        except ValueError as e:
            logging.warning(
                'Failed to parse date "{}" from headers: {}'.format(
                    error.response.headers["Date"], e
                )
            )
            now = time.time()

        timeout = max(
            int(error.response.headers["X-RateLimit-Reset"]) - now,
            transient_error_wait_time.next(),
        )

        logging.warning("Rate limiting hit, waiting for %s seconds", int(timeout))
        time.sleep(timeout)
    else:
        # If this is error from GitHub, there should be some explanation in response
        # content. Try to print it.
        try:
            github_msg = error.response.json()["message"]
        except Exception:
            github_msg = ""
        else:
            logging.error("GitHub: %s", github_msg)

        if "abuse detection" in github_msg:
            timeout = transient_error_wait_time.next()
            msg = "We have triggered abuse detection, waiting {} s...".format(timeout)
            logging.warning("%s", msg)
            time.sleep(timeout)
            return True

        return False

    return True


class Session(requests.Session):
    """
    A small extension for requests.Session that saves typing the host, calls
    raise_for_status() on responses by default, and allows specifying a default
    timeout.
    """

    def __init__(self, host, timeout=10):
        super().__init__()
        self.host = host
        self.timeout = timeout

    def post(self, path, json_, check=True):
        r = super().post(f"{self.host}/{path}", json=json_, timeout=self.timeout)
        if check:
            r.raise_for_status()
        if r.ok:
            transient_error_wait_time.reset()
        return r

    def get(self, path, check=True):
        r = super().get(f"{self.host}/{path}", timeout=self.timeout)
        if check:
            r.raise_for_status()
        if r.ok:
            transient_error_wait_time.reset()
        return r


class redirect_output:  # pylint: disable=invalid-name
    """
    A context manager that redirects stdout and stderr to a file.
    """

    def __init__(self, filename, mode="w"):
        self.filename = filename
        self.mode = mode
        self.redirect_file = None
        self.oldout = None
        self.olderr = None

    def __enter__(self):
        self.redirect_file = open(self.filename, self.mode, buffering=1)
        sys.stdout.flush()
        sys.stderr.flush()

        self.oldout = sys.stdout
        self.olderr = sys.stderr

        sys.stdout = self.redirect_file
        sys.stderr = self.redirect_file

        return self.redirect_file

    def __exit__(self, *exception):
        self.redirect_file.close()

        sys.stdout = self.oldout
        sys.stderr = self.olderr


class DontCacheStatuses(cachecontrol.heuristics.BaseHeuristic):
    """
    'Heuristics' for preventing caching of statuses.
    """

    def update_headers(self, response):
        # We need to remove 'max-age' from cache-control
        # TODO: figure out a way to discern responses for statuses query
        return {"cache-control": "private"}

    def warning(self, response):
        pass


def run(*argv, env=None, check=True, cwd=None, return_stdout=False):
    """
    Small wrapper around subprocess.run(), which prints the command to be
    executed and raises an exception by default.
    """
    runenv = dict(os.environ)
    if env:
        envrepr = " ".join(["=".join(e) for e in env.items()]) + " "
    else:
        envrepr = ""

    cwdrepr = "cd {}; ".format(shlex.quote(cwd)) if cwd else ""

    if env:
        runenv.update(env)

    print("+ " + cwdrepr + envrepr + " ".join(shlex.quote(a) for a in argv))
    stdout_val = sys.stdout
    if return_stdout:
        stdout_val = subprocess.PIPE

    return subprocess.run(
        argv,
        env=runenv,
        check=check,
        stdout=stdout_val,
        stderr=sys.stderr,
        cwd=cwd,
        encoding="utf-8",
    )


def fetch_image(url, cache):
    """
    Fetches an image from @url into @cache, if a file with the same name
    doesn't yet exist. There is no need for fancier caching, as image names are
    unique enough.

    Returns the full path to the image.
    """

    image_name = os.path.basename(urllib.parse.urlparse(url).path)
    path = os.path.join(cache, image_name)

    if not os.path.exists(path):
        print(f"Fetch {url}")

        image_tempfile = tempfile.NamedTemporaryFile(dir=cache, delete=False)
        try:
            request = urllib.request.urlopen(url)
            shutil.copyfileobj(request, image_tempfile)
            request.close()
        except Exception:  # pylint: disable=broad-except
            logging.warning(traceback.format_exc())
            os.unlink(image_tempfile.name)
            return None

        os.rename(image_tempfile.name, path)
    else:
        print(f"Use cached {image_name}")

    return path


class checkout_repository:  # pylint: disable=invalid-name
    """
    A context manager that shallowly checks out a github repository into a
    temporary directory.
    """

    def __init__(self, owner, repo, refspec):
        self.url = f"https://github.com/{owner}/{repo}"
        self.refspec = refspec
        self.dir = None

    def __enter__(self):
        self.dir = tempfile.TemporaryDirectory()

        run("git", "init", "--quiet", self.dir.name)
        run(
            "git",
            "-C",
            self.dir.name,
            "fetch",
            "--quiet",
            "--depth=1",
            self.url,
            self.refspec,
            env={"GIT_TERMINAL_PROMPT": "0"},
        )
        run("git", "-C", self.dir.name, "checkout", "--quiet", "FETCH_HEAD")

        return self.dir.name

    def __exit__(self, *exception):
        self.dir.cleanup()


class Task:
    """
    A task represents a single unit of work: test a specific pull request of a
    repository against an OS image.
    """

    def __init__(self, owner, repo, pull, head, pyghrepo, pyghcommit, image):
        self.owner = owner
        self.repo = repo
        self.pull = pull
        self.head = head
        self.pyghrepo = pyghrepo
        self.pyghcommit = pyghcommit
        self.image = image
        self.inventory = "/usr/share/ansible/inventory/standard-inventory-qcow2"

        self.id_ = f"pull-{owner}_{repo}-{pull}-{self.head[:7]}-" + image["name"]

    def __str__(self):
        return f"task {self.owner}/{self.repo}/{self.pull}:{self.image['name']}"

    def run(self, artifactsdir, cachedir, inventory=None):
        """
        Runs the task and puts results into @artifactsdir. Returns True if all
        tests succeeded.
        """

        if not inventory:
            inventory = self.inventory
        image_path = fetch_image(self.image["source"], cachedir)
        if not image_path:
            return False

        with checkout_repository(
            self.owner, self.repo, f"pull/{self.pull}/head"
        ) as sourcedir:
            # Generate a playbook with a single raw task to setup the image
            # (this usually means installing python2 on the guest for ansible)
            setup_file = os.path.join(sourcedir, "_setup.yml")

            # Playbook to fail when only localhost is available. This happens
            # when the inventory script fails. Then ansible-playbook would just
            # skip the tests with this warnings:
            #
            # [WARNING]: Unable to parse
            # /usr/share/ansible/inventory/standard-inventory-qcow2 as an
            # inventory source
            # [WARNING]: No inventory was parsed, only implicit localhost is
            # available
            # [WARNING]: provided hosts list is empty, only localhost is
            # available. Note that the implicit localhost does not match 'all'
            # [...]
            # skipping: no hosts matched
            inventory_fail_msg = "ERROR: Inventory is empty, tests did not run"
            fail_localhost = {
                "name": "Fail when only localhost is available",
                "hosts": "localhost",
                "gather_facts": False,
                "tasks": [
                    {"debug": {"var": "groups"}},
                    {
                        "fail": {"msg": inventory_fail_msg},
                        "when": ['groups["all"] == []'],
                    },
                ],
            }

            setup_plays = [fail_localhost]
            if "setup" in self.image:
                if isinstance(self.image["setup"], str):
                    play = {
                        "name": "Setup",
                        "hosts": "all",
                        "become": True,
                        "gather_facts": False,
                        "tasks": [{"raw": self.image["setup"]}],
                    }
                    setup_plays.append(play)
                else:
                    setup_plays.extend(self.image["setup"])

            with open(setup_file, "w") as outfile:
                yaml.dump(setup_plays, outfile)

            # Fedora's standard test invocation spec mandates running all
            # playbooks matching `tests/tests*.yml`, but linux-system-roles
            # used to be tested by running all playbooks `test/test_*.yml`.
            # Support both, but prefer the standard way. Can be removed once
            # all repos are moved over.
            playbookglob = f"{sourcedir}/tests/tests*.yml"
            playbooks = glob.glob(playbookglob)
            if not playbooks:
                playbooks = glob.glob(f"{sourcedir}/test/test_*.yml")

            if not playbooks:
                print(
                    f"No test playbooks found, please add at least one "
                    f"playbook that matches {playbookglob}."
                )
                return None

            ansible_log = f"{artifactsdir}/ansible.log"
            for playbook in sorted(playbooks):
                print(f"Testing {playbook}...", end="")
                with redirect_output(ansible_log, mode="a"):
                    # Use the qcow2 inventory from standard-test-roles, which
                    # boots a transient VM and runs the playbook against that.
                    # Create a fresh instance for each test playbook. However,
                    # we do need to run the setup (if it exists) in the same
                    # invocation of ansible-playbook, so that that it applies
                    # to the same VM as the test playbook.
                    result = run(
                        "ansible-playbook",
                        "-vv",
                        f"--inventory={inventory}",
                        setup_file,
                        playbook,
                        env={
                            "TEST_SUBJECTS": image_path,
                            "TEST_ARTIFACTS": artifactsdir,
                        },
                        check=False,
                        cwd=os.path.dirname(playbook),
                    )

                if result.returncode != 0:
                    with open(ansible_log, "r") as ansible_file:
                        for line in ansible_file:
                            if inventory_fail_msg in line:
                                print("ERROR: Inventory not properly set up")
                                return None

                    print("FAILURE")
                    return False

                print("SUCCESS")
            return True


def pull_ok_to_test(pyghrepo, pyghpull, author, head):
    """
    Returns True if we trust the pull request enough to run its code. It must
    either come from a collaborator of the repository (a github user with push
    access) or be marked with the "needs-ci" tag by a collaborator.
    """

    if pyghrepo.has_in_collaborators(author):
        logging.debug(f"PR is ok to test - {author} is a collaborator")
        return True

    # Check if a member commented with a command like
    # ci-check-commit:<commit-hash>
    # to allow to check this commit
    comments = pyghpull.get_issue_comments()
    whitelist_command = f"[citest commit:{head}]"
    for comment in comments:
        if comment.raw_data["author_association"] == "MEMBER":
            if whitelist_command in comment.body:
                logging.debug("PR is ok to test - whitelisted by member")
                return True

    labels = pyghpull.get_labels()
    for label in labels:
        if "needs-ci" in label.name:
            logging.debug("PR is ok to test - has needs-ci label")
            return True

    logging.debug("PR is not ok to test")
    return False


def get_statuses(commit):
    """
    Fetches all statuses of the given commit in a repository and returns a dict
    mapping context to its most recent status.
    """
    statuses = commit.get_statuses().reversed
    return {x.context: x for x in statuses}


def get_comment_commands(pyghpull):
    """ Returns dict with last occurrence of each command """
    commands = {
        COMMENT_CMD_TEST_ALL: "",
        COMMENT_CMD_TEST_BAD: "",
        COMMENT_CMD_TEST_PENDING: "",
    }
    comments = pyghpull.get_issue_comments()

    for comment in comments:
        updated_at = comment.updated_at.strftime("%Y%m%d-%H%M%S")
        for command in commands:
            if command in comment.body and commands[command] < updated_at:
                commands[command] = updated_at

    return commands


def get_status_context(variant, image_name, ansible_id):
    if variant:
        suffix = f" ({variant})"
    else:
        suffix = ""
    return f"{image_name}/{ansible_id}{suffix}"


def choose_task(gh, repos, images, ansible_id, args):
    """
    Collect tasks from open pull requests (one task for each image
    and each open pull).

    Return the first found task. The caller needs to provide shuffled
    repos/images to reduce the probability that other instances choose the same
    task.

    Returns None if there's nothing to do.
    """

    for owner, repo in repos:
        pyghrepo = gh.get_repo(f"{owner}/{repo}")
        pulls = [pr for pr in pyghrepo.get_pulls(state="open")]
        random.shuffle(pulls)
        for pull in pulls:
            logging.debug(f"Evaluating PR pull {pull.url}")
            author = pull.user.login
            head = pull.head.sha
            number = pull.number

            if not pull_ok_to_test(pyghrepo, pull, author, head):
                continue

            commit = pyghrepo.get_commit(head)
            statuses = get_statuses(commit)
            commands = get_comment_commands(pull)

            for image in images:
                status_context = get_status_context(
                    args.variant, image["name"], ansible_id
                )
                if status_context in statuses.keys():
                    status = statuses[status_context]
                else:
                    status = {}
                if check_commit_needs_testing(status, commands):
                    task = Task(owner, repo, number, head, pyghrepo, commit, image)
                    logging.debug(f"Testing PR {pull.url} with image {image['source']}")
                    return task
                else:
                    logging.debug(
                        f"Not testing PR {pull.url} with image {image['source']}"
                    )
            else:
                logging.debug("No images to use for testing PR {}".format(pull.url))
        else:
            logging.debug(f"No pull requests for {owner}/{repo}")


def check_commit_needs_testing(status, commands):
    """ Check if commit needs to be checked """

    # Check it, if there's no status for it yet
    if not status:
        logging.debug("PR will be tested because it has no status")
        return True

    # or the status is pending without a hostname in the description
    if status.state == "pending" and not status.description:
        logging.debug(
            "PR will be tested because it is in state 'pending' and has no description"
        )
        return True

    updated_at = status.updated_at.strftime("%Y%m%d-%H%M%S")
    # or a generic re-check was requested:
    if updated_at < commands[COMMENT_CMD_TEST_ALL]:
        logging.debug("PR will be tested because it was given comment to test all")
        return True

    # or the status is error or failure and a re-check was requested
    if (
        status.state in ("failure", "error")
        and updated_at < commands[COMMENT_CMD_TEST_BAD]
    ):
        logging.debug(
            "PR will be tested because it was given comment to retest failing tests"
        )
        return True

    # or the status is pending and a re-check was requested
    if status.state == "pending" and updated_at < commands[COMMENT_CMD_TEST_PENDING]:
        logging.debug(
            "PR will be tested because it was given comment to retest pending tests"
        )
        return True

    logging.debug("PR will not be tested because it does not meet criteria")
    return False


def scp(source, destination, secrets):
    """ Wrapper around scp to upload logs """
    result = run(
        "scp",
        "-o",
        f"IdentityFile {secrets}/id_rsa",
        "-o",
        f"UserKnownHostsFile {secrets}/known_hosts",
        "-rpq",
        source,
        destination,
        check=False,
    )
    return result.returncode == 0


def make_html(source_file):
    """ Create simple html file with navigation links from test.log  """
    links = {"index": ".", "ansible log": "ansible.log"}
    html_file = source_file + ".html"

    anchors = ""
    for name, target in links.items():
        a_html = "<a href='{}'>{}</a> ".format(html.escape(target), html.escape(name))
        anchors += a_html

    with open(source_file) as ifile:
        textdata = ifile.read()

    html_code = """<pre>{}</pre>
{}
    """.format(
        html.escape(textdata), anchors
    )

    with open(html_file, "w") as ofile:
        ofile.write(html_code)

    return html_file


def handle_task(gh, args, config, task, ansible_id, dry_run=False):
    """ Process a task """
    title = f"{HOSTNAME}: {task.owner}/{task.repo}: pull #{task.pull} "
    title += f'({task.head[:7]}) on {task.image["name"]}'
    logging.info(">>> " + title)

    abort = False

    start_time = datetime.datetime.utcnow()

    description = HOSTNAME + "@" + str(start_time)
    target_url = None
    state = None

    status_context = get_status_context(args.variant, task.image["name"], ansible_id)

    if not dry_run:
        logging.info("Claiming %s", task)
        # When running multiple instances of this script, there's a race
        # between choosing a task and setting the status on GitHub to "pending"
        # (there's no race-free way to only set the status for a context when
        # it doesn't yet exist).  Running the same tests multiple times does
        # not affect the resulting status on GitHub, as test runs should be
        # deterministic. We don't need to be perfect in avoiding it.
        #
        # Sleep for a couple of seconds after setting the task to "pending"
        # with our hostname as description. If the same description is set when
        # we wake up, we know that nobody else wants to do the same task and
        # can go ahead. Otherwise, choose something else.

        if task.pyghcommit:
            commit = task.pyghcommit
        else:
            commit = task.pyghrepo.get_commit(task.head)
        commit.create_status(
            state="pending", context=status_context, description=description
        )

    try:
        if not dry_run:
            time.sleep(random.randint(5, 20))

            statuses = get_statuses(commit)
            status = statuses[status_context]
            if status.description != description:
                logging.info(
                    "Skip: another instance is working on this task: "
                    f"{status.description} != {description}"
                )
                # avoid overwriting status from another instance
                dry_run = True
                return

        logging.info("Starting %s", task)
        timestamp = start_time.strftime("%Y%m%d-%H%M%S")

        workdir = tempfile.mkdtemp(prefix=f"linux-system-role-test-work-{task.id_}-")
        artifactsdir = f"{workdir}/artifacts"
        os.makedirs(artifactsdir)

        with redirect_output(f"{artifactsdir}/test.log"):
            print(title)
            print(len(title) * "=")
            print()
            try:
                task.inventory = args.inventory
                result = task.run(
                    f"{artifactsdir}", args.cache, inventory=args.inventory
                )
                if result:
                    state = "success"
                elif result is None:
                    state = "error"
                    description += ": Error running tests"
                else:
                    state = "failure"

            # Do not handle these exceptions
            except (KeyboardInterrupt, SystemExit):
                raise

            # pylint: disable=broad-except,invalid-name
            except Exception as e:
                if isinstance(e, OSError):
                    # No space left on device
                    # pylint: disable=no-member
                    if e.errno == 28:
                        abort = True

                print(traceback.format_exc())
                state = "error"
                description += ": Exception when running tests: " + str(e)
                logging.error(description)

        run("chmod", "a+rX", workdir)

        local_test_log = f"{artifactsdir}/test.log"

        if dry_run:
            logging.info(f"Artifacts kept at: {artifactsdir}")
            with open(local_test_log) as test_log:
                logging.info(test_log.read())
        else:
            make_html(local_test_log)

            if task.image.get("upload_results"):
                results_destination = config["results"]["destination"]
                results_url = config["results"]["public_url"]
                results_dir = f"{task.owner}-{task.repo}-{task.id_}-{timestamp}"

                if scp(workdir, f"{results_destination}/{results_dir}", args.secrets):
                    target_url = f"{results_url}/{results_dir}/artifacts/test.log.html"
                else:
                    state = "error"
                    description += ": Error uploading results"

            # FIXME: workdir might be kept when python crashes
            if not args.keep_results:
                shutil.rmtree(workdir)

    finally:
        duration = (datetime.datetime.utcnow() - start_time).seconds
        logging.info(
            "Finished in %d seconds %s: %s",
            duration,
            task,
            state if state and state != "pending" else "abandoned",
        )
        while not dry_run:
            try:
                commit.create_status
                (
                    state or "pending",
                    target_url,
                    description if state and state != "pending" else "",
                    status_context,
                )
                break
            except requests.exceptions.HTTPError as err:
                if not handle_transient_httperrors(err):
                    raise

    print()

    if abort:
        logging.critical("Fatal exception occured, aborting...")
        sys.exit(1)


def check_environment():
    """
    Check whether the environment is sane.
    Intent here is to fail early for example if /dev/kvm is not available.
    """
    if not os.access("/dev/kvm", os.R_OK | os.W_OK):
        logging.critical("test-harness needs access to /dev/kvm, aborting")
        sys.exit(1)


def setup_logging(config_logging):
    log_cfg_default = {
        "format": "%(asctime)s: %(levelname)s: %(message)s",
        "datefmt": "%Y-%m-%dT%H:%M:%S%z",
    }
    log_cfg = log_cfg_default.copy()
    log_cfg.update(
        (k, v)
        for k, v in config_logging.items()
        if k in {"format", "style", "datefmt", "level"}
    )

    try:
        # Add stderr handler
        stream_handler = logging.StreamHandler()
        stream_handler.setLevel(config_logging.get("stderr_level", logging.INFO))
        handlers = [stream_handler]

        if "filename" in config_logging:
            # as config may be shared between multiple hosts with shared storage,
            # we may need hostname in log name
            filename = config_logging["filename"].replace("HOSTNAME", HOSTNAME)
            file_handler = logging.FileHandler(filename)
            file_handler.setLevel(config_logging.get("file_level", logging.NOTSET))
            handlers.append(file_handler)

        logging.basicConfig(handlers=handlers, **log_cfg)
    except ValueError:
        logging.basicConfig(**log_cfg_default)
        logging.warning("Invalid logging config, using default", exc_info=True)


def get_ansible_version():
    """determine the version of Ansible"""
    rs = run("ansible", "--version", return_stdout=True)
    return rs.stdout.split()[1]


def main():
    signal.signal(signal.SIGTERM, sighandler_exit)
    print("Starting at {}".format(time.asctime()))

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--secrets",
        default=os.environ.get("TEST_HARNESS_SECRETS", "/secrets"),
        help="Directory with secrets",
    )
    parser.add_argument(
        "--config",
        default=os.environ.get("TEST_HARNESS_CONFIG", "/config"),
        help="Directory with config.json",
    )
    parser.add_argument(
        "--cache",
        default=os.environ.get("TEST_HARNESS_CACHE", "/cache"),
        help="Directory for caching VM images",
    )
    parser.add_argument(
        "--inventory",
        default=os.environ.get(
            "TEST_HARNESS_INVENTORY",
            "/usr/share/ansible/inventory/standard-inventory-qcow2",
        ),
        help="Inventory to use for VMs",
    )
    default_pull_request = os.environ.get("TEST_HARNESS_PULL_REQUEST", None)
    if default_pull_request:
        default_pull_request = default_pull_request.split(",")
    parser.add_argument(
        "pull_request",
        nargs="*",
        default=default_pull_request,
        help="Pull requests to test. Example: " "linux-system-roles/network/1",
    )
    parser.add_argument(
        "--use-images",
        default=os.environ.get("TEST_HARNESS_USE_IMAGES", "*"),
        help=(
            "Test pull request only against images matching these patterns.  "
            "This is a comma delimited list of fnmatch patterns which will be "
            "applied to each image name and source.  If any of the patterns "
            "match the name or source, the image will be included in testing."
        ),
    )
    parser.add_argument(
        "--dry-run",
        default=bool(strtobool(os.environ.get("TEST_HARNESS_DRY_RUN", "False"))),
        action="store_true",
        help="Do not update pull request status or upload artifacts",
    )
    parser.add_argument(
        "--keep-results",
        default=bool(strtobool(os.environ.get("TEST_HARNESS_KEEP_RESULTS", "False"))),
        action="store_true",
        help="For debugging - do not remove the temp results directory",
    )
    parser.add_argument(
        "--variant",
        default=os.environ.get("TEST_HARNESS_VARIANT", ""),
        help="Use this variant for PR test status e.g. staging",
    )
    parser.add_argument(
        "--repositories",
        default=os.environ.get("TEST_HARNESS_REPOSITORIES", None),
        help="Comma delimited list of repositories to override config",
    )
    parser.add_argument(
        "--max-num-statuses",
        type=int,
        default=int(os.environ.get("TEST_HARNESS_MAX_NUM_STATUSES", "60")),
        help="Number of statuses to retrieve - default 60 - github default is 30",
    )

    args = parser.parse_args()

    # default values for config
    config = {"repositories": [], "images": []}

    with open(args.config + "/config.json") as configfile:
        config.update(json.load(configfile))
        images = config["images"]
        random.shuffle(images)
        repos = [r.split("/") for r in config.get("repositories", [])]
        random.shuffle(repos)

    # this is used in PR status - context name
    if not args.variant:
        if "variant" not in config:
            # legacy support for old configs that still use "name"
            if config.get("name") == "linux-system-roles-test-staging":
                args.variant = "staging"
        else:
            args.variant = config["variant"]

    if args.repositories:
        repos = [r.split("/") for r in args.repositories.split(",")]
    setup_logging(config.get("logging", {}))

    check_environment()

    ansible_version = LooseVersion(get_ansible_version())
    logging.info("Using Ansible version {}".format(ansible_version))
    # this is used in PR status - context name
    ansible_id = f"ansible-{ansible_version.version[0]}.{ansible_version.version[1]}"

    image_patterns = args.use_images.split(",")
    # copy all images to test_images . . .
    test_images = dict([(image["name"], image) for image in images])
    # . . . then remove the ones that do not match the criteria
    for image in images:
        min_ansible_version = LooseVersion(image.get("min_ansible_version", "0"))
        if ansible_version < min_ansible_version:
            if image["name"] in test_images:
                logging.debug(
                    f"Image {image['name']} will not be used for testing because "
                    f"the minimum Ansible version {min_ansible_version} required "
                    f"by the image is greater than the version {ansible_version} "
                    "of Ansible used by the test."
                )
                del test_images[image["name"]]
            continue
        pattern_matched = False
        for pattern in image_patterns:
            if fnmatch.fnmatch(image["name"], pattern) or fnmatch.fnmatch(
                image["source"], pattern
            ):
                pattern_matched = True
                break
        if not pattern_matched:
            if image["name"] in test_images:
                logging.debug(
                    f"Image {image['name']} will not be used for testing "
                    "because neither the name nor the source match any "
                    f"of the image patterns in {image_patterns}"
                )
                del test_images[image["name"]]

    logging.info(
        f"Will test with the following image names: {list(test_images.keys())}"
    )
    test_images = list(test_images.values())
    with open(args.secrets + "/github-token") as tokenfile:
        token = tokenfile.read().strip()

    if args.dry_run:
        token = ""
    else:
        with open(args.secrets + "/github-token") as tokenfile:
            token = tokenfile.read().strip()

    if token:
        gh = Github(token, per_page=args.max_num_statuses)

    printed_waiting = False

    # Random delay at startup to prevent triggering abuse detection on GitHub
    # when multiple instances are started in same time
    if not args.dry_run:
        time.sleep(random.randint(0, 60))

    for pull_request in args.pull_request:
        logging.debug(f"Processing command line pull request {pull_request}")
        # supports to specify the PR with full URLs or just the path
        parsed_url = urllib.parse.urlparse(pull_request)
        owner, repo, pullnr = (
            parsed_url.path.strip("/").replace("/pull/", "/").split("/")
        )

        head = parsed_url.fragment
        pyghrepo = gh.get_repo(f"{owner}/{repo}")
        pull = pyghrepo.get_pull(int(pullnr))

        if not head:
            head = pull.head.sha
        number = pull.number

        for image in test_images:
            task = Task(owner, repo, number, head, pyghrepo, None, image)
            handle_task(gh, args, config, task, ansible_id, args.dry_run)

    while not args.pull_request:
        try:
            task = choose_task(gh, repos, test_images, ansible_id, args)
            if not task:
                if not printed_waiting:
                    logging.info(">>> No tasks. Waiting.")
                    printed_waiting = True
                else:
                    logging.debug(">>> Still no tasks. Waiting.")
                time.sleep(600)
                continue

            handle_task(gh, args, config, task, ansible_id, args.dry_run)
        except requests.exceptions.HTTPError as err:
            if not handle_transient_httperrors(err):
                raise

        # At this point, we have (probably) printed other messages, so
        # reset printed_waiting
        printed_waiting = False


if __name__ == "__main__":
    sys.exit(main())
