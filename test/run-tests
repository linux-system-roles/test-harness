#!/usr/bin/python3 -u
"""Test Harness for Linux System Roles"""

import argparse
import datetime
import fnmatch
import glob
import html
import json
import logging
import os
import errno
import random
import re
import signal
import shlex
import shutil
import socket
import subprocess
import sys
import tempfile
import time
import calendar
import traceback
import urllib.parse
import urllib.request

import requests
import cachecontrol
import cachecontrol.heuristics
import yaml

# this is what Ansible uses for version comparison
# use LooseVersion to handle alphas, betas, other pre-release versions
from distutils.version import LooseVersion
from distutils.util import strtobool
from checkmeta import role_supported
from compose2image import composeurl2images

HOSTNAME = socket.gethostname()

COMMENT_CMD_TEST_ALL = "[citest]"
COMMENT_CMD_TEST_PENDING = "[citest pending]"
COMMENT_CMD_TEST_BAD = "[citest bad]"

KVM_BACKEND = "kvm"
CITOOL_BACKEND = "citool"
CITOOL_COMMAND = "/entrypoint.sh"

# https://www.freedesktop.org/wiki/CommonExtendedAttributes/
URL_XATTR = "user.xdg.origin.url"
DATE_XATTR = "user.dublincore.date"


class TransientErrorWaitTime:
    TIME_DEFAULT = 30
    TIME_MAX = 600

    def __init__(self):
        self._value = self.TIME_DEFAULT

    def reset(self):
        self._value = self.TIME_DEFAULT

    def next(self):
        ret = self._value
        self._value = min(self._value * 2, self.TIME_MAX)
        return ret


transient_error_wait_time = TransientErrorWaitTime()


def sighandler_exit(signo, frame):
    logging.info(f"Received {signal.Signals(signo).name}, exiting...")
    sys.exit(0)


def handle_transient_httperrors(error):
    """
    Sleep when a transient error occured.

    Return True if slept, False otherwise
    """

    # Handle transient server-side errors
    if error.response.status_code in (500, 502, 503, 504):
        timeout = transient_error_wait_time.next()
        logging.info(
            ">>> Server returned {} {}, waiting {} s...".format(
                error.response.status_code, error.response.reason, timeout
            )
        )
        time.sleep(timeout)
    # Handle rate limiting
    elif (
        error.response.status_code == 403
        and "X-RateLimit-Remaining" in error.response.headers
        and int(error.response.headers["X-RateLimit-Remaining"]) == 0
    ):
        try:
            now = calendar.timegm(
                time.strptime(
                    error.response.headers["Date"], "%a, %d %b %Y %H:%M:%S %Z"
                )
            )
        except ValueError as e:
            logging.warning(
                'Failed to parse date "{}" from headers: {}'.format(
                    error.response.headers["Date"], e
                )
            )
            now = time.time()

        timeout = max(
            int(error.response.headers["X-RateLimit-Reset"]) - now,
            transient_error_wait_time.next(),
        )

        logging.warning("Rate limiting hit, waiting for %s seconds", int(timeout))
        time.sleep(timeout)
    else:
        # If this is error from GitHub, there should be some explanation in response
        # content. Try to print it.
        try:
            github_msg = error.response.json()["message"]
        except Exception:
            github_msg = ""
        else:
            logging.error("GitHub: %s", github_msg)

        if "abuse detection" in github_msg:
            timeout = transient_error_wait_time.next()
            msg = "We have triggered abuse detection, waiting {} s...".format(timeout)
            logging.warning("%s", msg)
            time.sleep(timeout)
            return True

        return False

    return True


class Session(requests.Session):
    """
    A small extension for requests.Session that saves typing the host, calls
    raise_for_status() on responses by default, and allows specifying a default
    timeout.
    """

    def __init__(self, host, timeout=10):
        super().__init__()
        self.host = host
        self.timeout = timeout

    def post(self, path, json_, check=True):
        r = super().post(f"{self.host}/{path}", json=json_, timeout=self.timeout)
        if check:
            r.raise_for_status()
        if r.ok:
            transient_error_wait_time.reset()
        return r

    def get(self, path, check=True):
        r = super().get(f"{self.host}/{path}", timeout=self.timeout)
        if check:
            r.raise_for_status()
        if r.ok:
            transient_error_wait_time.reset()
        return r


class redirect_output:  # pylint: disable=invalid-name
    """
    A context manager that redirects stdout and stderr to a file.
    """

    def __init__(self, filename, mode="w"):
        self.filename = filename
        self.mode = mode
        self.redirect_file = None
        self.oldout = None
        self.olderr = None

    def __enter__(self):
        self.redirect_file = open(self.filename, self.mode, buffering=1)
        sys.stdout.flush()
        sys.stderr.flush()

        self.oldout = sys.stdout
        self.olderr = sys.stderr

        sys.stdout = self.redirect_file
        sys.stderr = self.redirect_file

        return self.redirect_file

    def __exit__(self, *exception):
        self.redirect_file.close()

        sys.stdout = self.oldout
        sys.stderr = self.olderr


class DontCacheStatuses(cachecontrol.heuristics.BaseHeuristic):
    """
    'Heuristics' for preventing caching of statuses.
    """

    def update_headers(self, response):
        # We need to remove 'max-age' from cache-control
        # TODO: figure out a way to discern responses for statuses query
        return {"cache-control": "private"}

    def warning(self, response):
        pass


def run(*argv, env=None, check=True, cwd=None, return_stdout=False):
    """
    Small wrapper around subprocess.run(), which prints the command to be
    executed and raises an exception by default.
    """
    runenv = dict(os.environ)
    if env:
        envrepr = " ".join(["=".join(e) for e in env.items()]) + " "
    else:
        envrepr = ""

    cwdrepr = "cd {}; ".format(shlex.quote(cwd)) if cwd else ""

    if env:
        runenv.update(env)

    print("+ " + cwdrepr + envrepr + " ".join(shlex.quote(a) for a in argv))
    stdout_val = sys.stdout
    if return_stdout:
        stdout_val = subprocess.PIPE

    result = subprocess.run(
        argv,
        env=runenv,
        check=check,
        stdout=stdout_val,
        stderr=sys.stderr,
        cwd=cwd,
        encoding="utf-8",
    )
    logging.debug(f"run: result of {argv} was {result}")
    return result


def get_metadata_from_file(path, attr_key):
    try:
        mdbytes = os.getxattr(path, attr_key)
    except OSError as e:
        if e.errno == errno.ENODATA:
            return None
        raise
    return os.fsdecode(mdbytes)


def image_source_last_modified_by_file_metadata(path):
    return get_metadata_from_file(path, DATE_XATTR) if os.path.exists(path) else ""


def origurl(path):
    """
    Returns the original URL that a given file was downloaded from.
    """
    return get_metadata_from_file(path, URL_XATTR)


def get_metadata_from_url(url, metadata_key):
    with urllib.request.urlopen(url) as url_response:
        return url_response.getheader(metadata_key)


def fetch_image(url, cache, label):
    """
    Fetches an image from @url into @cache as @label if a file with the
    same name downloaded from the same URL doesn't yet exist. There is
    no need for fancier caching, as image URLs are unique enough.

    Labels are not unique enough, because the URL corresponding to
    the label may get updated. And using a filename derived from URL
    would lead to leftover image files filling up the cache directory,
    as nobody would delete them when the URL changes.

    Returns the full path to the image.
    """

    original_name = os.path.basename(urllib.parse.urlparse(url).path)
    nameroot, suffix = os.path.splitext(original_name)
    image_name = label + suffix
    path = os.path.join(cache, image_name)
    image_last_modified_by_src = get_metadata_from_url(url, "Last-Modified")
    image_last_modified_by_file = image_source_last_modified_by_file_metadata(path)

    if (
        not os.path.exists(path)
        or url != origurl(path)
        or image_last_modified_by_src != image_last_modified_by_file
    ):
        print(f"Fetch {url}")

        image_tempfile = tempfile.NamedTemporaryFile(dir=cache, delete=False)
        try:
            request = urllib.request.urlopen(url)
            shutil.copyfileobj(request, image_tempfile)
            request.close()
        except Exception:  # pylint: disable=broad-except
            logging.warning(traceback.format_exc())
            os.unlink(image_tempfile.name)
            return None

        os.setxattr(image_tempfile.name, URL_XATTR, os.fsencode(url))
        os.setxattr(
            image_tempfile.name, DATE_XATTR, os.fsencode(image_last_modified_by_src)
        )
        os.rename(image_tempfile.name, path)
    else:
        print(f"Use cached {image_name}")

    return path


class checkout_repository:  # pylint: disable=invalid-name
    """
    A context manager that shallowly checks out a github repository into a
    temporary directory.
    """

    def __init__(self, owner, repo, refspec):
        self.url = f"https://github.com/{owner}/{repo}"
        self.refspec = refspec
        self.dir = None

    def __enter__(self):
        self.dir = tempfile.TemporaryDirectory()

        run("git", "init", "--quiet", self.dir.name)
        run(
            "git",
            "-C",
            self.dir.name,
            "fetch",
            "--quiet",
            "--depth=1",
            self.url,
            self.refspec,
            env={"GIT_TERMINAL_PROMPT": "0"},
        )
        run("git", "-C", self.dir.name, "checkout", "--quiet", "FETCH_HEAD")

        return self.dir.name

    def __exit__(self, *exception):
        self.dir.cleanup()


class Task:
    """
    A task represents a single unit of work: test a specific pull request of a
    repository against an OS image.
    """

    def __init__(self, owner, repo, pull, head, image):
        self.owner = owner
        self.repo = repo
        self.pull = pull
        self.head = head
        self.image = image
        self.inventory = "/usr/share/ansible/inventory/standard-inventory-qcow2"

        self.id_ = f"pull-{owner}_{repo}-{pull}-{self.head[:7]}-" + image["name"]

    def __str__(self):
        return f"task {self.owner}/{self.repo}/{self.pull}:{self.image['name']}"

    def get_url(self):
        source = self.image.get("source")
        if source:
            return source
        compose_url = self.image.get("compose")
        if compose_url:
            variant = self.image.get("variant")
            image_urls = composeurl2images(compose_url, "x86_64", variant)
            if len(image_urls) == 1:
                return image_urls[0]
            else:
                if image_urls:
                    logging.error(
                        f"ERROR: Multiple images found: {image_urls}"
                        "in compose {compose_url}"
                    )
                else:
                    logging.error(f"ERROR: no image found in compose {compose_url}")
        else:
            logging.error(
                "ERROR: neither source nor compose specified"
                f"in image {self.image['name']}"
            )

    def is_role_distro_supported(self):
        """
        Return True if image is supported. Otherwise False.
        """
        distro, version = self.image["name"].split("-")[:2]
        if version == "x":
            image_filename = os.path.basename(
                urllib.parse.urlparse(self.get_url()).path
            )
            # We expect the first number in the URL image filename to be the major
            # version
            version = re.search(r"\d+", image_filename).group()

        try:
            with checkout_repository(
                self.owner, self.repo, f"pull/{self.pull}/head"
            ) as repo_tmp_dir:
                is_supported = role_supported(
                    f"{ repo_tmp_dir }/meta/main.yml", distro, version
                )
                if is_supported:
                    logging.debug(
                        f"The role { self.repo } supports platform "
                        f"{ self.image['name'] }."
                    )
                else:
                    logging.debug(
                        f"The role { self.repo } does not support platform "
                        f"{ self.image['name'] }. Skipping."
                    )
                return is_supported
        except Exception:
            logging.warning(
                "WARNING: Cannot open metafile meta/main.yml. Skipping all images."
            )
            return None

    def run(
        self,
        artifactsdir,
        private_artifactsdir,
        cachedir,
        backend,
        inventory=None,
        test_collections=False,
        keep_results=False,
    ):
        """
        Runs the task and puts results into @artifactsdir. Puts non-public
        artifacts into @private_artifactsdir.  Returns True if all
        tests succeeded.
        """

        if not inventory:
            inventory = self.inventory

        if backend == KVM_BACKEND:
            image_url = self.get_url()
            if not image_url:
                return None
            image_path = fetch_image(image_url, cachedir, self.image["name"])
            if not image_path:
                return None

        with checkout_repository(
            self.owner, self.repo, f"pull/{self.pull}/head"
        ) as sourcedir:
            # If test_collections is true, converts the role to the collections format.
            if test_collections:
                os.environ["COLLECTION_SRC_PATH"] = sourcedir
                os.environ["COLLECTION_ROLE"] = self.repo
                os.environ["COLLECTION_DEST_PATH"] = tempfile.TemporaryDirectory().name
                # The tests dir is copied to the temporary dir
                # which is outside of the collections.
                os.environ[
                    "COLLECTION_TESTS_DEST_PATH"
                ] = tempfile.TemporaryDirectory().name
                logging.debug(f"PYTHONPATH - {sys.path}")
                from importlib import import_module

                _lsr = import_module("lsr_role2collection")
                _lsr.role2collection()
                collection_paths = (
                    os.environ["COLLECTION_DEST_PATH"]
                    + ":~/.ansible/collections:/usr/share/ansible/collections"
                )

            # Generate a playbook with a single raw task to setup the image
            # (this usually means installing python2 on the guest for ansible)
            setup_file = os.path.join(sourcedir, "_setup.yml")

            # Playbook to fail when only localhost is available. This happens
            # when the inventory script fails. Then ansible-playbook would just
            # skip the tests with this warnings:
            #
            # [WARNING]: Unable to parse
            # /usr/share/ansible/inventory/standard-inventory-qcow2 as an
            # inventory source
            # [WARNING]: No inventory was parsed, only implicit localhost is
            # available
            # [WARNING]: provided hosts list is empty, only localhost is
            # available. Note that the implicit localhost does not match 'all'
            # [...]
            # skipping: no hosts matched
            inventory_fail_msg = "ERROR: Inventory is empty, tests did not run"
            fail_localhost = {
                "name": "Fail when only localhost is available",
                "hosts": "localhost",
                "gather_facts": False,
                "tasks": [
                    {"debug": {"var": "groups"}},
                    {
                        "fail": {"msg": inventory_fail_msg},
                        "when": ['groups["all"] == []'],
                    },
                ],
            }

            setup_plays = [fail_localhost]
            if "setup" in self.image:
                if isinstance(self.image["setup"], str):
                    play = {
                        "name": "Setup",
                        "hosts": "all",
                        "become": True,
                        "gather_facts": False,
                        "tasks": [{"raw": self.image["setup"]}],
                    }
                    setup_plays.append(play)
                else:
                    setup_plays.extend(self.image["setup"])

            with open(setup_file, "w") as outfile:
                yaml.dump(setup_plays, outfile)

            # Install tests requirements if there are any.
            requirements_file = os.path.join(sourcedir, "tests", "requirements.yml")
            if os.path.exists(requirements_file):
                roles_path = os.path.join(sourcedir, "tests", "roles")
                if not os.path.exists(roles_path):
                    os.mkdir(roles_path)
                run(
                    "ansible-galaxy",
                    "install",
                    "--roles-path",
                    roles_path,
                    "-r",
                    requirements_file,
                )

            # Fedora's standard test invocation spec mandates running all
            # playbooks matching `tests/tests*.yml`, but linux-system-roles
            # used to be tested by running all playbooks `test/test_*.yml`.
            # Support both, but prefer the standard way. Can be removed once
            # all repos are moved over.
            playbookglob = f"{sourcedir}/tests/tests*.yml"
            playbooks = glob.glob(playbookglob)

            if not playbooks:
                playbooks = glob.glob(f"{sourcedir}/test/test_*.yml")

            if not playbooks:
                print(
                    f"No test playbooks found, please add at least one "
                    f"playbook that matches {playbookglob}."
                )
                return None

            _playbook_set = [{"is_collection": False, "playbooks": playbooks}]

            if test_collections:
                playbookglob = (
                    os.environ["COLLECTION_TESTS_DEST_PATH"]
                    + "/tests/"
                    + self.repo
                    + "/tests*.yml"
                )
                _playbook_set.append(
                    {"is_collection": True, "playbooks": glob.glob(playbookglob)}
                )

            ansible_log = f"{artifactsdir}/ansible.log"
            if backend == CITOOL_BACKEND:
                output_log = f"{private_artifactsdir}/citool.log"
            else:
                output_log = ansible_log
            for pset in _playbook_set:
                for playbook in sorted(pset["playbooks"]):
                    print(f"Testing {playbook}...", end="")
                    with redirect_output(output_log, mode="a"):
                        # Use the qcow2 inventory from standard-test-roles, which
                        # boots a transient VM and runs the playbook against that.
                        # Create a fresh instance for each test playbook. However,
                        # we do need to run the setup (if it exists) in the same
                        # invocation of ansible-playbook, so that that it applies
                        # to the same VM as the test playbook.
                        if backend == CITOOL_BACKEND:
                            testenv = {}
                            if test_collections and pset["is_collection"]:
                                testenv = {
                                    "ANSIBLE_COLLECTIONS_PATHS": collection_paths,
                                }
                            result = run(
                                CITOOL_COMMAND,
                                "-o",
                                f"{private_artifactsdir}/citool-debug.log",
                                "ansible",
                                "rules-engine",
                                "guest-setup",
                                "--playbooks",
                                setup_file,
                                "guess-environment",
                                "--image-method=force",
                                "--image",
                                self.image["openstack_image"],
                                "--distro-method=force",
                                "--distro",
                                self.image["openstack_image"],
                                "--compose-method=force",
                                "--compose",
                                self.image["openstack_image"],
                                "openstack",
                                "test-scheduler-sti",
                                "--playbook",
                                playbook,
                                "test-scheduler",
                                "test-schedule-runner-sti",
                                "test-schedule-runner",
                                "test-schedule-report",
                                env=testenv,
                                check=False,
                                cwd=os.path.dirname(playbook),
                            )
                        elif backend == KVM_BACKEND:
                            testenv = {
                                "TEST_SUBJECTS": image_path,
                                "TEST_ARTIFACTS": artifactsdir,
                            }
                            if test_collections and pset["is_collection"]:
                                testenv["ANSIBLE_COLLECTIONS_PATHS"] = collection_paths
                            result = run(
                                "ansible-playbook",
                                "-vv",
                                f"--inventory={inventory}",
                                setup_file,
                                playbook,
                                env=testenv,
                                check=False,
                                cwd=os.path.dirname(playbook),
                            )

                        else:
                            assert False, "unreachable"

                    if backend == CITOOL_BACKEND:
                        ptrn = re.compile(
                            f"{playbook}] Ansible logs are in (.+/ansible-output.txt)"
                        )
                        # parse the ansible output file location from citool-debug.log
                        ansible_output = ""
                        with open(
                            f"{private_artifactsdir}/citool-debug.log"
                        ) as citooldbg:
                            for line in citooldbg:
                                mtch = ptrn.search(line)
                                if mtch:
                                    ansible_output = mtch.group(1)
                                    break
                        # ansible_output is relative to /WORKDIR
                        if ansible_output:
                            # filter out garbage in ansible_output
                            with open(f"/WORKDIR/{ansible_output}") as inf:
                                with open(ansible_log, "w") as outf:
                                    docopy = False
                                    for line in inf:
                                        if line.startswith("---v---v---v---v---v---"):
                                            docopy = True
                                        elif line.startswith("---^---^---^---^---^---"):
                                            break
                                        elif docopy:
                                            outf.write(line)
                        else:
                            logging.error(
                                "ERROR: Could not find location of ansible output "
                                "in citool-debug.log"
                            )
                            print("FAILURE")
                            return False

                    if result.returncode != 0:
                        with open(ansible_log, "r") as ansible_file:
                            for line in ansible_file:
                                if inventory_fail_msg in line:
                                    print("ERROR: Inventory not properly set up")
                                    return None

                        print("FAILURE")
                        logging.debug(f"test failed for {artifactsdir}")
                        if test_collections and not keep_results:
                            cleanup_collection_tempdirs(remove_script=False)
                        return False

                    print("SUCCESS")
            if test_collections and not keep_results:
                cleanup_collection_tempdirs(remove_script=False)
            logging.debug(f"test passed for {artifactsdir}")
            return True


def pull_ok_to_test(gh, owner, repo, pull, author, head):
    """
    Returns True if we trust the pull request enough to run its code. It must
    either come from a collaborator of the repository (a github user with push
    access) or be marked with the "needs-ci" tag by a collaborator.
    """

    result = gh.get(f"repos/{owner}/{repo}/collaborators/{author}", check=False)
    if result.status_code == 204:
        logging.debug(
            "PR is ok to test - is a collaborator "
            f"repos/{owner}/{repo}/collaborators/{author}"
        )
        return True

    # Check if a member commented with a command like
    # ci-check-commit:<commit-hash>
    # to allow to check this commit
    comments = get_comments(gh, owner, repo, pull)
    whitelist_command = f"[citest commit:{head}]"
    for comment in comments:
        if comment["author_association"] == "MEMBER":
            if whitelist_command in comment["body"]:
                logging.debug("PR is ok to test - whitelisted by member")
                return True

    result = gh.get(f"repos/{owner}/{repo}/issues/{pull}/labels")
    if result.status_code == 200 and "needs-ci" in (x["name"] for x in result.json()):
        logging.debug("PR is ok to test - has needs-ci label")
        return True
    logging.debug("PR is not ok to test")


def get_statuses(gh, owner, repo, sha, max_num_statuses):
    """
    Fetches all statuses of the given commit in a repository and returns a dict
    mapping context to its most recent status.
    Will return at most max_num_statuses.   By default, github will return 30, but
    some of our PRs have more than that.
    """
    # https://developer.github.com/v3/repos/statuses/#get-the-combined-status-for-a-specific-ref
    status = gh.get(
        f"repos/{owner}/{repo}/commits/{sha}/status?per_page={max_num_statuses}"
    ).json()
    if status["total_count"] > len(status["statuses"]):
        # error - there are too many statuses
        if max_num_statuses <= len(status["statuses"]):
            logging.error(
                f"Total number of statuses {status['total_count']} exceeds "
                f"max_num_statuses {max_num_statuses} statuses for "
                f"{owner}/{repo}/commits/{sha} - please increase max_num_statuses"
            )
        else:
            # hit hard limit - will have to use paging :-(
            logging.error(
                f"Total number of statuses {status['total_count']} exceeds "
                f"max_num_statuses {max_num_statuses} statuses for "
                f"{owner}/{repo}/commits/{sha} - need to use paging"
            )
    return {x["context"]: x for x in status["statuses"]}


def get_comments(gh, owner, repo, pullnr, after=""):
    """Get comments for a pull request, optionally after a certain time"""
    result = gh.get(f"repos/{owner}/{repo}/issues/{pullnr}/comments")
    comments = []
    if result.status_code == 200:
        comments = result.json()

    new_comments = []
    for comment in comments:
        comment_update = comment["updated_at"]
        if comment_update > after:
            new_comments.append(comment)
    comments = new_comments

    return comments


def get_comment_commands(gh, owner, repo, pullnr):
    """Returns dict with last occurence of each command"""
    commands = {
        COMMENT_CMD_TEST_ALL: "",
        COMMENT_CMD_TEST_BAD: "",
        COMMENT_CMD_TEST_PENDING: "",
    }
    comments = get_comments(gh, owner, repo, pullnr)

    for comment in comments:
        for command in commands:
            if command in comment["body"] and commands[command] < comment["updated_at"]:
                commands[command] = comment["updated_at"]

    return commands


def get_status_context(variant, image_name, ansible_id):
    if variant:
        suffix = f" ({variant})"
    else:
        suffix = ""
    return f"{image_name}/{ansible_id}{suffix}"


def choose_task(gh, repos, images, config, ansible_id, args):
    """
    Collect tasks from open pull requests (one task for each image
    and each open pull).

    Return the first found task. The caller needs to provide shuffled
    repos/images to reduce the probability that other instances choose the same
    task.

    Returns None if there's nothing to do.
    """

    for owner, repo in repos:
        pulls = gh.get(f"repos/{owner}/{repo}/pulls").json()
        random.shuffle(pulls)
        for pull in pulls:
            logging.debug("Evaluating PR {}".format(pull["url"]))
            author = pull["user"]["login"]
            head = pull["head"]["sha"]
            number = pull["number"]

            if not pull_ok_to_test(gh, owner, repo, number, author, head):
                continue

            statuses = get_statuses(gh, owner, repo, head, args.max_num_statuses)
            commands = get_comment_commands(gh, owner, repo, number)

            for image in images:
                status_context = get_status_context(
                    args.variant, image["name"], ansible_id
                )
                status = statuses.get(status_context)

                if check_commit_needs_testing(status, commands):
                    task = Task(owner, repo, number, head, image)
                    logging.debug(
                        f"Testing PR {pull['url']} with image {image['name']}"
                    )
                    return task
                else:
                    logging.debug(
                        f"Not testing PR {pull['url']} with image {image['name']}"
                    )
            else:
                logging.debug("No images to use for testing PR {}".format(pull["url"]))
        else:
            logging.debug(f"No pull requests for {owner}/{repo}")


def check_commit_needs_testing(status, commands):
    """Check if commit needs to be checked"""

    # Check it, if there's no status for it yet
    if not status:
        logging.debug("PR will be tested because it has no status")
        return True

    # or the status is pending without a hostname in the description
    if status["state"] == "pending" and not status.get("description"):
        logging.debug(
            "PR will be tested because it is in state 'pending' and has no description"
        )
        return True

    # or a generic re-check was requested:
    if status["updated_at"] < commands[COMMENT_CMD_TEST_ALL]:
        logging.debug("PR will be tested because it was given comment to test all")
        return True

    # or the status is error or failure and a re-check was requested
    if (
        status["state"] in ("failure", "error")
        and status["updated_at"] < commands[COMMENT_CMD_TEST_BAD]
    ):
        logging.debug(
            "PR will be tested because it was given comment to retest failing tests"
        )
        return True

    # or the status is pending and a re-check was requested
    if (
        status["state"] == "pending"
        and status["updated_at"] < commands[COMMENT_CMD_TEST_PENDING]
    ):
        logging.debug(
            "PR will be tested because it was given comment to retest pending tests"
        )
        return True

    logging.debug("PR will not be tested because it does not meet criteria")
    return False


def scp(source, destination, secrets):
    """Wrapper around scp to upload logs"""
    result = run(
        "scp",
        "-o",
        f"IdentityFile {secrets}/id_rsa",
        "-o",
        f"UserKnownHostsFile {secrets}/known_hosts",
        "-rpq",
        source,
        destination,
        check=False,
    )
    return result.returncode == 0


def make_html(source_file):
    """Create simple html file with navigation links from test.log"""
    links = {"index": ".", "ansible log": "ansible.log"}
    html_file = source_file + ".html"

    anchors = ""
    for name, target in links.items():
        a_html = "<a href='{}'>{}</a> ".format(html.escape(target), html.escape(name))
        anchors += a_html

    with open(source_file) as ifile:
        textdata = ifile.read()

    html_code = """<pre>{}</pre>
{}
    """.format(
        html.escape(textdata), anchors
    )

    with open(html_file, "w") as ofile:
        ofile.write(html_code)

    return html_file


def cleanup_collection_tempdirs(remove_script=True):
    if os.path.isdir(os.environ["COLLECTION_TESTS_DEST_PATH"]):
        shutil.rmtree(os.environ["COLLECTION_TESTS_DEST_PATH"])
    if os.path.isdir(os.environ["COLLECTION_DEST_PATH"]):
        shutil.rmtree(os.environ["COLLECTION_DEST_PATH"])
    if remove_script:
        for pp in sys.path:
            if os.path.isdir(pp) and str.startswith(pp, "/tmp/tmp"):
                shutil.rmtree(pp)


def handle_task(
    gh, args, config, task, ansible_id, dry_run=False, test_collections=False
):
    """Process a task"""
    title = f"{HOSTNAME}: {task.owner}/{task.repo}: pull #{task.pull} "
    title += f'({task.head[:7]}) on {task.image["name"]}'
    logging.info(">>> " + title)

    abort = False

    start_time = datetime.datetime.utcnow()

    description = HOSTNAME + "@" + str(start_time)
    target_url = None
    state = None

    status_context = get_status_context(args.variant, task.image["name"], ansible_id)

    if not dry_run:
        logging.info("Claiming %s", task)
        # When running multiple instances of this script, there's a race
        # between choosing a task and setting the status on GitHub to "pending"
        # (there's no race-free way to only set the status for a context when
        # it doesn't yet exist).  Running the same tests multiple times does
        # not affect the resulting status on GitHub, as test runs should be
        # deterministic. We don't need to be perfect in avoiding it.
        #
        # Sleep for a couple of seconds after setting the task to "pending"
        # with our hostname as description. If the same description is set when
        # we wake up, we know that nobody else wants to do the same task and
        # can go ahead. Otherwise, choose something else.

        gh.post(
            f"repos/{task.owner}/{task.repo}/statuses/{task.head}",
            {"context": status_context, "state": "pending", "description": description},
        )

    try:
        if not dry_run:
            time.sleep(random.randint(5, 20))

            statuses = get_statuses(
                gh, task.owner, task.repo, task.head, args.max_num_statuses
            )
            status = statuses.get(status_context)
            if status["description"] != description:
                logging.info(
                    "Skip: another instance is working on this task: "
                    f"{status['description']}"
                )
                # avoid overwriting status from another instance
                dry_run = True
                return

        role_supported_state = task.is_role_distro_supported()
        if role_supported_state is False:
            state = "success"
            description = (
                f"The role does not support this platform. Skipping. {description}"
            )
            return
        elif role_supported_state is None:
            state = "failure"
            description = (
                f"The role does not contain meta/main.yml file! Skipping. {description}"
            )
            return

        logging.info("Starting %s", task)
        timestamp = start_time.strftime("%Y%m%d-%H%M%S")

        workdir = tempfile.mkdtemp(prefix=f"linux-system-role-test-work-{task.id_}-")
        private_workdir = tempfile.mkdtemp(
            prefix=f"linux-system-role-test-work-private-{task.id_}-"
        )
        artifactsdir = f"{workdir}/artifacts"
        os.makedirs(artifactsdir)
        private_artifactsdir = f"{private_workdir}/artifacts"
        os.makedirs(private_artifactsdir)

        with redirect_output(f"{artifactsdir}/test.log"):
            print(title)
            print(len(title) * "=")
            print()
            try:
                task.inventory = args.inventory
                result = task.run(
                    artifactsdir,
                    private_artifactsdir,
                    args.cache,
                    args.backend,
                    inventory=args.inventory,
                    test_collections=test_collections,
                    keep_results=args.keep_results,
                )
                logging.debug(f"task result {result} for {artifactsdir}")
                if result:
                    state = "success"
                elif result is None:
                    state = "error"
                    description += ": Error running tests"
                else:
                    state = "failure"

            # Do not handle these exceptions
            except (KeyboardInterrupt, SystemExit):
                logging.debug(f"task terminated unexpectedly for {artifactsdir}")
                raise

            # pylint: disable=broad-except,invalid-name
            except Exception as e:
                if isinstance(e, OSError):
                    # No space left on device
                    # pylint: disable=no-member
                    if e.errno == 28:
                        abort = True

                print(traceback.format_exc())
                state = "error"
                description += ": Exception when running tests: " + str(e)
                logging.error(description)

        run("chmod", "a+rX", workdir)

        local_test_log = f"{artifactsdir}/test.log"

        if dry_run:
            logging.info(
                f"Artifacts kept at: {artifactsdir}; "
                f"private artifacts at {private_artifactsdir}"
            )
            with open(local_test_log) as test_log:
                logging.info(test_log.read())
        else:
            make_html(local_test_log)

            if task.image.get("upload_results"):
                results_destination = config["results"]["destination"]
                results_url = config["results"]["public_url"]
                results_dir = f"{task.owner}-{task.repo}-{task.id_}-{timestamp}"

                if scp(workdir, f"{results_destination}/{results_dir}", args.secrets):
                    target_url = f"{results_url}/{results_dir}/artifacts/test.log.html"
                else:
                    logging.error("Could not upload results - check ssh keys")
                    description += ": Error uploading results"
            else:
                # move files to private_artifactsdir
                for ff in os.listdir(artifactsdir):
                    shutil.move(f"{artifactsdir}/{ff}", f"{private_artifactsdir}/{ff}")

            # FIXME: workdir might be kept when python crashes
            if not args.keep_results:
                shutil.rmtree(workdir)
                shutil.rmtree(private_workdir)

    finally:
        duration = (datetime.datetime.utcnow() - start_time).seconds
        logging.info(
            "Finished in %d seconds %s: %s - %s",
            duration,
            task,
            state if state and state != "pending" else "abandoned",
            description,
        )
        while not dry_run:
            try:
                gh.post(
                    f"repos/{task.owner}/{task.repo}/statuses/{task.head}",
                    {
                        "context": status_context,
                        "state": state or "pending",
                        "target_url": target_url,
                        "description": description
                        if state and state != "pending"
                        else "",
                    },
                )
                break
            except requests.exceptions.HTTPError as err:
                if not handle_transient_httperrors(err):
                    raise

    print()

    if abort:
        logging.critical("Fatal exception occured, aborting...")
        sys.exit(1)


def check_environment(args):
    """
    Check whether the environment is sane.
    Intent here is to fail early for example if /dev/kvm is not available.
    """
    if args.backend == KVM_BACKEND:
        if not os.access("/dev/kvm", os.R_OK | os.W_OK):
            logging.critical("test-harness needs access to /dev/kvm, aborting")
            sys.exit(1)

    elif args.backend == CITOOL_BACKEND:
        try:
            run(CITOOL_COMMAND, "-V", "-o", "/tmp/citool-debug.log")
            run("ansible", "--version")
        except Exception:
            logging.critical(
                f"test-harness needs installed citool command {CITOOL_COMMAND}, "
                "aborting"
            )
            sys.exit(1)


def setup_logging(config_logging):
    log_cfg_default = {
        "format": "%(asctime)s: %(levelname)s: %(message)s",
        "datefmt": "%Y-%m-%dT%H:%M:%S%z",
    }
    log_cfg = log_cfg_default.copy()
    log_cfg.update(
        (k, v)
        for k, v in config_logging.items()
        if k in {"format", "style", "datefmt", "level"}
    )

    initial_log_message = ""
    try:
        # Add stderr handler
        stream_handler = logging.StreamHandler()
        stream_handler.setLevel(config_logging.get("stderr_level", logging.INFO))
        handlers = [stream_handler]

        if "filename" in config_logging:
            # as config may be shared between multiple hosts with shared storage,
            # we may need hostname in log name
            filename = config_logging["filename"].replace("HOSTNAME", HOSTNAME)
            try:
                file_handler = logging.FileHandler(filename)
                file_handler.setLevel(config_logging.get("file_level", logging.NOTSET))
                handlers.append(file_handler)
            except PermissionError:
                # running as non-root user, but trying to write to /var/log, usually -
                # log the error, skip logging to file
                initial_log_message = (
                    f"could not open {filename} as uid "
                    f"{os.geteuid()}: PermissionError: "
                    "will not log to file"
                )

        logging.basicConfig(handlers=handlers, **log_cfg)
        if initial_log_message:
            logging.warning(initial_log_message)
    except ValueError:
        logging.basicConfig(**log_cfg_default)
        logging.warning("Invalid logging config, using default", exc_info=True)


def get_ansible_version():
    """determine the version of Ansible"""
    rs = run("ansible", "--version", return_stdout=True)
    return rs.stdout.split()[1]


def main():
    signal.signal(signal.SIGTERM, sighandler_exit)
    print("Starting at {}".format(time.asctime()))

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--secrets",
        default=os.environ.get("TEST_HARNESS_SECRETS", "/secrets"),
        help="Directory with secrets",
    )
    parser.add_argument(
        "--config",
        default=os.environ.get("TEST_HARNESS_CONFIG", "/config"),
        help="Directory with config.json",
    )
    parser.add_argument(
        "--cache",
        default=os.environ.get("TEST_HARNESS_CACHE", "/cache"),
        help="Directory for caching VM images",
    )
    parser.add_argument(
        "--inventory",
        default=os.environ.get(
            "TEST_HARNESS_INVENTORY",
            "/usr/share/ansible/inventory/standard-inventory-qcow2",
        ),
        help="Inventory to use for VMs",
    )
    default_pull_request = os.environ.get("TEST_HARNESS_PULL_REQUEST", None)
    if default_pull_request:
        default_pull_request = default_pull_request.split(",")
    parser.add_argument(
        "pull_request",
        nargs="*",
        default=default_pull_request,
        help="Pull requests to test. Example: " "linux-system-roles/network/1",
    )
    parser.add_argument(
        "--use-images",
        default=os.environ.get("TEST_HARNESS_USE_IMAGES", "*"),
        help=(
            "Test pull request only against images matching these patterns.  "
            "This is a comma delimited list of fnmatch patterns which will be "
            "applied to each image name and source.  If any of the patterns "
            "match the name or source, the image will be included in testing."
        ),
    )
    parser.add_argument(
        "--dry-run",
        default=bool(strtobool(os.environ.get("TEST_HARNESS_DRY_RUN", "False"))),
        action="store_true",
        help="Do not update pull request status or upload artifacts",
    )
    parser.add_argument(
        "--keep-results",
        default=bool(strtobool(os.environ.get("TEST_HARNESS_KEEP_RESULTS", "False"))),
        action="store_true",
        help="For debugging - do not remove the temp results directory",
    )
    parser.add_argument(
        "--variant",
        default=os.environ.get("TEST_HARNESS_VARIANT", ""),
        help="Use this variant for PR test status e.g. staging",
    )
    parser.add_argument(
        "--repositories",
        default=os.environ.get("TEST_HARNESS_REPOSITORIES", None),
        help="Comma delimited list of repositories to override config",
    )
    parser.add_argument(
        "--max-num-statuses",
        type=int,
        default=int(os.environ.get("TEST_HARNESS_MAX_NUM_STATUSES", "99")),
        help="Number of statuses to retrieve - default 99 - github default is 30",
    )
    parser.add_argument(
        "--backend",
        default=os.environ.get("TEST_HARNESS_BACKEND", KVM_BACKEND),
        choices=[KVM_BACKEND, CITOOL_BACKEND],
        help="Backend for testing",
    )
    parser.add_argument(
        "--collections",
        default=bool(strtobool(os.environ.get("TEST_HARNESS_COLLECTIONS", "False"))),
        action="store_true",
        help="Run CI tests in the roles and collections formats",
    )

    args = parser.parse_args()

    # default values for config
    config = {"repositories": [], "images": []}

    with open(args.config + "/config.json") as configfile:
        config.update(json.load(configfile))
        images = config["images"]
        random.shuffle(images)
        repos = [r.split("/") for r in config.get("repositories", [])]
        random.shuffle(repos)

    # this is used in PR status - context name
    if not args.variant:
        if "variant" not in config:
            # legacy support for old configs that still use "name"
            if config.get("name") == "linux-system-roles-test-staging":
                args.variant = "staging"
        else:
            args.variant = config["variant"]

    if args.repositories:
        repos = [r.split("/") for r in args.repositories.split(",")]
    setup_logging(config.get("logging", {}))

    check_environment(args)

    ansible_version = LooseVersion(get_ansible_version())
    logging.info("Using Ansible version {}".format(ansible_version))
    # this is used in PR status - context name
    ansible_id = f"ansible-{ansible_version.version[0]}.{ansible_version.version[1]}"

    # If test_collections is true, download lsr_role2collection.py
    test_collections = args.collections
    if test_collections:
        base_url = (
            "https://raw.githubusercontent.com/linux-system-roles"
            "/auto-maintenance/master"
        )
        r2c_url = f"{base_url}/lsr_role2collection.py"
        r2c = requests.get(r2c_url, allow_redirects=True)
        r2c_path = tempfile.mkdtemp()
        role2collection_path = r2c_path + "/lsr_role2collection.py"
        open(role2collection_path, "wb").write(r2c.content)
        runtime_url = f"{base_url}/lsr_role2collection/runtime.yml"
        runtime = requests.get(runtime_url, allow_redirects=True)
        runtime_path = r2c_path + "/runtime.yml"
        open(runtime_path, "wb").write(runtime.content)
        logging.debug(f"Write lsr_role2collection and runtime.yml to {r2c_path}")
        sys.path.append(r2c_path)
        logging.debug(f"Appended {r2c_path} to PYTHONPATH")
        os.environ["COLLECTION_SRC_OWNER"] = "linux-system-roles"
        os.environ["COLLECTION_META_RUNTIME"] = runtime_path

    image_patterns = args.use_images.split(",")
    # copy all images to test_images . . .
    test_images = dict([(image["name"], image) for image in images])
    # . . . then remove the ones that do not match the criteria
    for image in images:
        min_ansible_version = LooseVersion(image.get("min_ansible_version", "0"))
        if ansible_version < min_ansible_version:
            if image["name"] in test_images:
                logging.debug(
                    f"Image {image['name']} will not be used for testing because "
                    f"the minimum Ansible version {min_ansible_version} required "
                    f"by the image is greater than the version {ansible_version} "
                    "of Ansible used by the test."
                )
                del test_images[image["name"]]
            continue

        if not image.get("openstack_image") and args.backend == CITOOL_BACKEND:
            logging.debug(
                f"Image {image['name']} will not be used for testing because it is not "
                "supported in Citool backend."
            )
            del test_images[image["name"]]
            continue

        pattern_matched = False
        for pattern in image_patterns:
            if fnmatch.fnmatch(image["name"], pattern) or fnmatch.fnmatch(
                image.get("source", ""), pattern
            ):
                pattern_matched = True
                break
        if not pattern_matched:
            if image["name"] in test_images:
                logging.debug(
                    f"Image {image['name']} will not be used for testing "
                    "because neither the name nor the source match any "
                    f"of the image patterns in {image_patterns}"
                )
                del test_images[image["name"]]

    logging.info(
        f"Will test with the following image names: {list(test_images.keys())}"
    )
    test_images = list(test_images.values())
    if args.dry_run:
        token = ""
    else:
        with open(args.secrets + "/github-token") as tokenfile:
            token = tokenfile.read().strip()

    gh = Session("https://api.github.com")
    gh.headers.update(
        {
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "linux-system-roles/test",
        }
    )

    if token:
        gh.headers.update({"Authorization": f"token {token}"})

    # Use CacheControl.
    # Requests on keep-alive connections fail when the remote has closed a
    # connection before we've noticed and sent another request. Thus,
    # always retry sending requests once.
    gh.mount(
        "https://",
        cachecontrol.CacheControlAdapter(max_retries=1, heuristic=DontCacheStatuses()),
    )

    printed_waiting = False

    # Random delay at startup to prevent triggering abuse detection on GitHub
    # when multiple instances are started in same time
    if not args.dry_run:
        time.sleep(random.randint(0, 60))

    for pull_request in args.pull_request:
        logging.debug(f"Processing command line pull request {pull_request}")
        # supports to specify the PR with full URLs or just the path
        parsed_url = urllib.parse.urlparse(pull_request)
        owner, repo, pullnr = (
            parsed_url.path.strip("/").replace("/pull/", "/").split("/")
        )

        head = parsed_url.fragment
        pull = gh.get(f"repos/{owner}/{repo}/pulls/{pullnr}").json()

        if not head:
            head = pull["head"]["sha"]
        number = pull["number"]

        for image in test_images:
            task = Task(owner, repo, number, head, image)
            handle_task(
                gh,
                args,
                config,
                task,
                ansible_id,
                dry_run=args.dry_run,
                test_collections=test_collections,
            )

    while not args.pull_request:
        try:
            task = choose_task(gh, repos, test_images, config, ansible_id, args)
            if not task:
                if not printed_waiting:
                    logging.info(">>> No tasks. Waiting.")
                    printed_waiting = True
                else:
                    logging.debug(">>> Still no tasks. Waiting.")
                time.sleep(600)
                continue

            handle_task(
                gh,
                args,
                config,
                task,
                ansible_id,
                dry_run=args.dry_run,
                test_collections=test_collections,
            )
        except requests.exceptions.HTTPError as err:
            if not handle_transient_httperrors(err):
                raise

        # At this point, we have (probably) printed other messages, so
        # reset printed_waiting
        printed_waiting = False

    if test_collections and not args.keep_results:
        cleanup_collection_tempdirs(remove_script=True)


if __name__ == "__main__":
    sys.exit(main())
