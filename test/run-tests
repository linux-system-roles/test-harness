#!/usr/bin/python3 -u
"""Test Harness for Linux System Roles"""

import argparse
import datetime
import fnmatch
import glob
import html
import json
import logging
import os
import random
import signal
import shlex
import shutil
import socket
import subprocess
import sys
import tempfile
import time
import calendar
import traceback
import urllib.parse
import urllib.request

import requests
import cachecontrol
import cachecontrol.heuristics
import yaml

HOSTNAME = socket.gethostname()

COMMENT_CMD_TEST_ALL = "[citest]"
COMMENT_CMD_TEST_PENDING = "[citest pending]"
COMMENT_CMD_TEST_BAD = "[citest bad]"


class TransientErrorWaitTime:
    TIME_DEFAULT = 30
    TIME_MAX = 600

    def __init__(self):
        self._value = self.TIME_DEFAULT

    def reset(self):
        self._value = self.TIME_DEFAULT

    def next(self):
        ret = self._value
        self._value = min(self._value * 2, self.TIME_MAX)
        return ret


transient_error_wait_time = TransientErrorWaitTime()


def sighandler_exit(signo, frame):
    print(f"Received {signal.Signals(signo).name}, exiting...", file=sys.stderr)
    sys.exit(0)


def handle_transient_httperrors(error):
    """
    Sleep when a transient error occured.

    Return True if slept, False otherwise
    """

    # Handle transient server-side errors
    if error.response.status_code in (500, 502, 503, 504):
        timeout = transient_error_wait_time.next()
        print(
            ">>> Server returned {} {}, waiting {} s...".format(
                error.response.status_code, error.response.reason, timeout
            )
        )
        time.sleep(timeout)
    # Handle rate limiting
    elif (
        error.response.status_code == 403
        and "X-RateLimit-Remaining" in error.response.headers
        and int(error.response.headers["X-RateLimit-Remaining"]) == 0
    ):
        try:
            now = calendar.timegm(
                time.strptime(
                    error.response.headers["Date"], "%a, %d %b %Y %H:%M:%S %Z"
                )
            )
        except ValueError as e:
            print(
                'W: Failed to parse date "{}" from headers: {}'.format(
                    error.response.headers["Date"], e
                )
            )
            now = time.time()

        timeout = max(
            int(error.response.headers["X-RateLimit-Reset"]) - now,
            transient_error_wait_time.next(),
        )

        logging.info("Rate limiting hit, waiting for %s seconds", int(timeout))
        print(f">>> Rate limiting hit, waiting for {int(timeout)} seconds...")
        time.sleep(timeout)
    else:
        # If this is error from GitHub, there should be some explanation in response
        # content. Try to print it.
        try:
            github_msg = error.response.json()["message"]
        except Exception:
            github_msg = ""
        else:
            print("W: GitHub: " + github_msg)
            logging.error("GitHub: %s", github_msg)

        if "abuse detection" in github_msg:
            timeout = transient_error_wait_time.next()
            msg = "We have triggered abuse detection, waiting {} s...".format(timeout)
            print(">>> {}".format(msg))
            logging.warning("%s", msg)
            time.sleep(timeout)
            return True

        return False

    return True


class Session(requests.Session):
    """
    A small extension for requests.Session that saves typing the host, calls
    raise_for_status() on responses by default, and allows specifying a default
    timeout.
    """

    def __init__(self, host, timeout=10):
        super().__init__()
        self.host = host
        self.timeout = timeout

    def post(self, path, json_, check=True):
        r = super().post(f"{self.host}/{path}", json=json_, timeout=self.timeout)
        if check:
            r.raise_for_status()
        if r.ok:
            transient_error_wait_time.reset()
        return r

    def get(self, path, check=True):
        r = super().get(f"{self.host}/{path}", timeout=self.timeout)
        if check:
            r.raise_for_status()
        if r.ok:
            transient_error_wait_time.reset()
        return r


class redirect_output:  # pylint: disable=invalid-name
    """
    A context manager that redirects stdout and stderr to a file.
    """

    def __init__(self, filename, mode="w"):
        self.filename = filename
        self.mode = mode
        self.redirect_file = None
        self.oldout = None
        self.olderr = None

    def __enter__(self):
        self.redirect_file = open(self.filename, self.mode, buffering=1)
        sys.stdout.flush()
        sys.stderr.flush()

        self.oldout = sys.stdout
        self.olderr = sys.stderr

        sys.stdout = self.redirect_file
        sys.stderr = self.redirect_file

        return self.redirect_file

    def __exit__(self, *exception):
        self.redirect_file.close()

        sys.stdout = self.oldout
        sys.stderr = self.olderr


class DontCacheStatuses(cachecontrol.heuristics.BaseHeuristic):
    """
    'Heuristics' for preventing caching of statuses.
    """

    def update_headers(self, response):
        # We need to remove 'max-age' from cache-control
        # TODO: figure out a way to discern responses for statuses query
        return {"cache-control": "private"}

    def warning(self, response):
        pass


def run(*argv, env=None, check=True, cwd=None):
    """
    Small wrapper around subprocess.run(), which prints the command to be
    executed and raises an exception by default.
    """
    runenv = dict(os.environ)
    if env:
        envrepr = " ".join(["=".join(e) for e in env.items()]) + " "
    else:
        envrepr = ""

    cwdrepr = "cd {}; ".format(shlex.quote(cwd)) if cwd else ""

    if env:
        runenv.update(env)

    print("+ " + cwdrepr + envrepr + " ".join(shlex.quote(a) for a in argv))

    return subprocess.run(
        argv, env=runenv, check=check, stdout=sys.stdout, stderr=sys.stderr, cwd=cwd
    )


def fetch_image(url, cache):
    """
    Fetches an image from @url into @cache, if a file with the same name
    doesn't yet exist. There is no need for fancier caching, as image names are
    unique enough.

    Returns the full path to the image.
    """

    image_name = os.path.basename(urllib.parse.urlparse(url).path)
    path = os.path.join(cache, image_name)

    if not os.path.exists(path):
        print(f"Fetch {url}")

        image_tempfile = tempfile.NamedTemporaryFile(dir=cache, delete=False)
        try:
            request = urllib.request.urlopen(url)
            shutil.copyfileobj(request, image_tempfile)
            request.close()
        except Exception:  # pylint: disable=broad-except
            logging.warning(traceback.format_exc())
            os.unlink(image_tempfile.name)
            return None

        os.rename(image_tempfile.name, path)
    else:
        print(f"Use cached {image_name}")

    return path


class checkout_repository:  # pylint: disable=invalid-name
    """
    A context manager that shallowly checks out a github repository into a
    temporary directory.
    """

    def __init__(self, owner, repo, refspec):
        self.url = f"https://github.com/{owner}/{repo}"
        self.refspec = refspec
        self.dir = None

    def __enter__(self):
        self.dir = tempfile.TemporaryDirectory()

        run("git", "init", "--quiet", self.dir.name)
        run(
            "git",
            "-C",
            self.dir.name,
            "fetch",
            "--quiet",
            "--depth=1",
            self.url,
            self.refspec,
            env={"GIT_TERMINAL_PROMPT": "0"},
        )
        run("git", "-C", self.dir.name, "checkout", "--quiet", "FETCH_HEAD")

        return self.dir.name

    def __exit__(self, *exception):
        self.dir.cleanup()


class Task:
    """
    A task represents a single unit of work: test a specific pull request of a
    repository against an OS image.
    """

    def __init__(self, owner, repo, pull, head, image):
        self.owner = owner
        self.repo = repo
        self.pull = pull
        self.head = head
        self.image = image
        self.inventory = "/usr/share/ansible/inventory/standard-inventory-qcow2"

        self.id_ = f"pull-{owner}_{repo}-{pull}-{self.head[:7]}-" + image["name"]

    def __str__(self):
        return f"task {self.owner}/{self.repo}/{self.pull}:{self.image['name']}"

    def run(self, artifactsdir, cachedir, inventory=None):
        """
        Runs the task and puts results into @artifactsdir. Returns True if all
        tests succeeded.
        """

        if not inventory:
            inventory = self.inventory
        image_path = fetch_image(self.image["source"], cachedir)
        if not image_path:
            return False

        with checkout_repository(
            self.owner, self.repo, f"pull/{self.pull}/head"
        ) as sourcedir:
            # Generate a playbook with a single raw task to setup the image
            # (this usually means installing python2 on the guest for ansible)
            setup_file = os.path.join(sourcedir, "_setup.yml")

            # Playbook to fail when only localhost is available. This happens
            # when the inventory script fails. Then ansible-playbook would just
            # skip the tests with this warnings:
            #
            # [WARNING]: Unable to parse
            # /usr/share/ansible/inventory/standard-inventory-qcow2 as an
            # inventory source
            # [WARNING]: No inventory was parsed, only implicit localhost is
            # available
            # [WARNING]: provided hosts list is empty, only localhost is
            # available. Note that the implicit localhost does not match 'all'
            # [...]
            # skipping: no hosts matched
            inventory_fail_msg = "ERROR: Inventory is empty, tests did not run"
            fail_localhost = {
                "name": "Fail when only localhost is available",
                "hosts": "localhost",
                "gather_facts": False,
                "tasks": [
                    {"debug": {"var": "groups"}},
                    {
                        "fail": {"msg": inventory_fail_msg},
                        "when": ['groups["all"] == []'],
                    },
                ],
            }

            setup_plays = [fail_localhost]
            if "setup" in self.image:
                if isinstance(self.image["setup"], str):
                    play = {
                        "name": "Setup",
                        "hosts": "all",
                        "become": True,
                        "gather_facts": False,
                        "tasks": [{"raw": self.image["setup"]}],
                    }
                    setup_plays.append(play)
                else:
                    setup_plays.extend(self.image["setup"])

            with open(setup_file, "w") as outfile:
                yaml.dump(setup_plays, outfile)

            # Fedora's standard test invocation spec mandates running all
            # playbooks matching `tests/tests*.yml`, but linux-system-roles
            # used to be tested by running all playbooks `test/test_*.yml`.
            # Support both, but prefer the standard way. Can be removed once
            # all repos are moved over.
            playbookglob = f"{sourcedir}/tests/tests*.yml"
            playbooks = glob.glob(playbookglob)
            if not playbooks:
                playbooks = glob.glob(f"{sourcedir}/test/test_*.yml")

            if not playbooks:
                print(
                    f"No test playbooks found, please add at least one "
                    f"playbook that matches {playbookglob}."
                )
                return None

            ansible_log = f"{artifactsdir}/ansible.log"
            for playbook in sorted(playbooks):
                print(f"Testing {playbook}...", end="")
                with redirect_output(ansible_log, mode="a"):
                    # Use the qcow2 inventory from standard-test-roles, which
                    # boots a transient VM and runs the playbook against that.
                    # Create a fresh instance for each test playbook. However,
                    # we do need to run the setup (if it exists) in the same
                    # invocation of ansible-playbook, so that that it applies
                    # to the same VM as the test playbook.
                    result = run(
                        "ansible-playbook",
                        "-vv",
                        f"--inventory={inventory}",
                        setup_file,
                        playbook,
                        env={
                            "TEST_SUBJECTS": image_path,
                            "TEST_ARTIFACTS": artifactsdir,
                        },
                        check=False,
                        cwd=os.path.dirname(playbook),
                    )

                if result.returncode != 0:
                    with open(ansible_log, "r") as ansible_file:
                        for line in ansible_file:
                            if inventory_fail_msg in line:
                                print("ERROR: Inventory not properly set up")
                                return None

                    print("FAILURE")
                    return False

                print("SUCCESS")
            return True


def pull_ok_to_test(gh, owner, repo, pull, author, head):
    """
    Returns True if we trust the pull request enough to run its code. It must
    either come from a collaborator of the repository (a github user with push
    access) or be marked with the "needs-ci" tag by a collaborator.
    """

    result = gh.get(f"repos/{owner}/{repo}/collaborators/{author}", check=False)
    if result.status_code == 204:
        return True

    # Check if a member commented with a command like
    # ci-check-commit:<commit-hash>
    # to allow to check this commit
    comments = get_comments(gh, owner, repo, pull)
    whitelist_command = f"[citest commit:{head}]"
    for comment in comments:
        if comment["author_association"] == "MEMBER":
            if whitelist_command in comment["body"]:
                return True

    result = gh.get(f"repos/{owner}/{repo}/issues/{pull}/labels")
    if result.status_code == 200 and "needs-ci" in (x["name"] for x in result.json()):
        return True


def get_statuses(gh, owner, repo, sha):
    """
    Fetches all statuses of the given commit in a repository and returns a dict
    mapping context to its most recent status.
    """

    statuses = {}
    for status in gh.get(f"repos/{owner}/{repo}/statuses/{sha}").json():
        # only add the first occurence of a context, since the result is in
        # reverse chronological order:
        # https://developer.github.com/v3/repos/statuses/#list-statuses-for-a-specific-ref
        statuses.setdefault(status["context"], status)

    return statuses


def get_comments(gh, owner, repo, pullnr, after=""):
    """ Get comments for a pull request, optionally after a certain time """
    result = gh.get(f"repos/{owner}/{repo}/issues/{pullnr}/comments")
    comments = []
    if result.status_code == 200:
        comments = result.json()

    new_comments = []
    for comment in comments:
        comment_update = comment["updated_at"]
        if comment_update > after:
            new_comments.append(comment)
    comments = new_comments

    return comments


def get_comment_commands(gh, owner, repo, pullnr):
    """ Returns dict with last occurence of each command """
    commands = {
        COMMENT_CMD_TEST_ALL: "",
        COMMENT_CMD_TEST_BAD: "",
        COMMENT_CMD_TEST_PENDING: "",
    }
    comments = get_comments(gh, owner, repo, pullnr)

    for comment in comments:
        for command in commands:
            if command in comment["body"] and commands[command] < comment["updated_at"]:
                commands[command] = comment["updated_at"]

    return commands


def choose_task(gh, repos, images, config):
    """
    Collect tasks from open pull requests (one task for each image
    and each open pull).

    Return the first found task. The caller needs to provide shuffled
    repos/images to reduce the probability that other instances choose the same
    task.

    Returns None if there's nothing to do.
    """

    for owner, repo in repos:
        pulls = gh.get(f"repos/{owner}/{repo}/pulls").json()
        random.shuffle(pulls)
        for pull in pulls:
            author = pull["user"]["login"]
            head = pull["head"]["sha"]
            number = pull["number"]

            if not pull_ok_to_test(gh, owner, repo, number, author, head):
                continue

            statuses = get_statuses(gh, owner, repo, head)
            commands = get_comment_commands(gh, owner, repo, number)

            for image in images:
                status_context = f"{config['name']}/{image['name']}"
                status = statuses.get(status_context)

                if check_commit_needs_testing(status, commands):
                    task = Task(owner, repo, number, head, image)
                    return task


def check_commit_needs_testing(status, commands):
    """ Check if commit needs to be checked """

    # Check it, if there's no status for it yet
    if not status:
        return True

    # or the status is pending without a hostname in the description
    if status["state"] == "pending" and not status.get("description"):
        return True

    # or a generic re-check was requested:
    if status["updated_at"] < commands[COMMENT_CMD_TEST_ALL]:
        return True

    # or the status is error or failure and a re-check was requested
    if (
        status["state"] in ("failure", "error")
        and status["updated_at"] < commands[COMMENT_CMD_TEST_BAD]
    ):
        return True

    # or the status is pending and a re-check was requested
    if (
        status["state"] == "pending"
        and status["updated_at"] < commands[COMMENT_CMD_TEST_PENDING]
    ):
        return True

    return False


def scp(source, destination, secrets):
    """ Wrapper around scp to upload logs """
    result = run(
        "scp",
        "-o",
        f"IdentityFile {secrets}/id_rsa",
        "-o",
        f"UserKnownHostsFile {secrets}/known_hosts",
        "-rpq",
        source,
        destination,
        check=False,
    )
    return result.returncode == 0


def make_html(source_file):
    """ Create simple html file with navigation links from test.log  """
    links = {"index": ".", "ansible log": "ansible.log"}
    html_file = source_file + ".html"

    anchors = ""
    for name, target in links.items():
        a_html = "<a href='{}'>{}</a> ".format(html.escape(target), html.escape(name))
        anchors += a_html

    with open(source_file) as ifile:
        textdata = ifile.read()

    html_code = """<pre>{}</pre>
{}
    """.format(
        html.escape(textdata), anchors
    )

    with open(html_file, "w") as ofile:
        ofile.write(html_code)

    return html_file


def handle_task(gh, args, config, task, dry_run=False):
    """ Process a task """
    title = f"{HOSTNAME}: {task.owner}/{task.repo}: pull #{task.pull} "
    title += f'({task.head[:7]}) on {task.image["name"]}'
    print(">>>", title)

    abort = False

    description = HOSTNAME + "@" + str(datetime.datetime.utcnow())
    target_url = None
    state = None

    status_context = f"{config['name']}/{task.image['name']}"

    if not dry_run:
        logging.info("Claiming %s", task)
        # When running multiple instances of this script, there's a race
        # between choosing a task and setting the status on GitHub to "pending"
        # (there's no race-free way to only set the status for a context when
        # it doesn't yet exist).  Running the same tests multiple times does
        # not affect the resulting status on GitHub, as test runs should be
        # deterministic. We don't need to be perfect in avoiding it.
        #
        # Sleep for a couple of seconds after setting the task to "pending"
        # with our hostname as description. If the same description is set when
        # we wake up, we know that nobody else wants to do the same task and
        # can go ahead. Otherwise, choose something else.

        gh.post(
            f"repos/{task.owner}/{task.repo}/statuses/{task.head}",
            {"context": status_context, "state": "pending", "description": description},
        )

    try:
        if not dry_run:
            time.sleep(random.randint(5, 20))

            statuses = get_statuses(gh, task.owner, task.repo, task.head)
            status = statuses.get(status_context)
            if status["description"] != description:
                print(
                    f"Skip: another instance is working on this task: "
                    + status["description"]
                )
                print()
                # avoid overwriting status from another instance
                dry_run = True
                return

        logging.info("Starting %s", task)
        timestamp = datetime.datetime.utcnow().strftime("%Y%m%d-%H%M%S")

        workdir = tempfile.mkdtemp(prefix=f"linux-system-role-test-work-{task.id_}-")
        artifactsdir = f"{workdir}/artifacts"
        os.makedirs(artifactsdir)

        with redirect_output(f"{artifactsdir}/test.log"):
            print(title)
            print(len(title) * "=")
            print()
            try:
                task.inventory = args.inventory
                result = task.run(
                    f"{artifactsdir}", args.cache, inventory=args.inventory
                )
                if result:
                    state = "success"
                elif result is None:
                    state = "error"
                    description += ": Error running tests"
                else:
                    state = "failure"

            # Do not handle these exceptions
            except (KeyboardInterrupt, SystemExit):
                raise

            # pylint: disable=broad-except,invalid-name
            except Exception as e:
                if isinstance(e, OSError):
                    # No space left on device
                    # pylint: disable=no-member
                    if e.errno == 28:
                        abort = True

                print(traceback.format_exc())
                state = "error"
                description += ": Exception when running tests: " + str(e)

        run("chmod", "a+rX", workdir)

        local_test_log = f"{artifactsdir}/test.log"

        if dry_run:
            print(f"Artifacts kept at: {artifactsdir}")
            with open(local_test_log) as test_log:
                print(test_log.read())
        else:
            make_html(local_test_log)

            if task.image.get("upload_results"):
                results_destination = config["results"]["destination"]
                results_url = config["results"]["public_url"]
                results_dir = f"{task.owner}-{task.repo}-{task.id_}-{timestamp}"

                if scp(workdir, f"{results_destination}/{results_dir}", args.secrets):
                    target_url = f"{results_url}/{results_dir}/artifacts/test.log.html"
                else:
                    state = "error"
                    description += ": Error uploading results"

            # FIXME: workdir might be kept when python crashes
            shutil.rmtree(workdir)

    finally:
        logging.info(
            "Finished %s: %s",
            task,
            state if state and state != "pending" else "abandoned",
        )
        while not dry_run:
            try:
                gh.post(
                    f"repos/{task.owner}/{task.repo}/statuses/{task.head}",
                    {
                        "context": status_context,
                        "state": state or "pending",
                        "target_url": target_url,
                        "description": description
                        if state and state != "pending"
                        else "",
                    },
                )
                break
            except requests.exceptions.HTTPError as err:
                if not handle_transient_httperrors(err):
                    raise

    print()

    if abort:
        print("Fatal exception occured, aborting...")
        sys.exit(1)


def check_environment():
    """
    Check whether the environment is sane.
    Intent here is to fail early for example if /dev/kvm is not available.
    """
    if not os.access("/dev/kvm", os.R_OK | os.W_OK):
        print("test-harness needs access to /dev/kvm, aborting")
        sys.exit(1)


def setup_logging(config_logging):
    log_cfg_default = {
        "format": "%(asctime)s: %(levelname)s: %(message)s",
        "datefmt": "%Y-%m-%dT%H:%M:%S%z",
    }

    log_cfg = log_cfg_default.copy()
    log_cfg.update(config_logging)

    # as config may be shared between multiple hosts with shared storage, we
    # need hostname in log name
    if "filename" in log_cfg:
        if "HOSTNAME" in log_cfg["filename"]:
            log_cfg["filename"] = log_cfg["filename"].replace("HOSTNAME", HOSTNAME)
        else:
            log_cfg["filename"] = log_cfg["filename"] + HOSTNAME

    try:
        logging.basicConfig(**log_cfg)
    except ValueError:
        logging.basicConfig(**log_cfg_default)
        logging.warning("Invalid logging config, using default", exc_info=True)


def main():
    signal.signal(signal.SIGTERM, sighandler_exit)

    parser = argparse.ArgumentParser()
    parser.add_argument("--secrets", default="/secrets", help="Directory with secrets")
    parser.add_argument(
        "--config", default="/config", help="Directory with config.json"
    )
    parser.add_argument(
        "--cache", default="/cache", help="Directory for caching VM images"
    )
    parser.add_argument(
        "--inventory",
        help="Inventory to use for VMs",
        default="/usr/share/ansible/inventory/standard-inventory-qcow2",
    )
    parser.add_argument(
        "pull_request",
        nargs="*",
        default=None,
        help="Pull requests to test. Example: " "linux-system-roles/network/1",
    )
    parser.add_argument(
        "--use-images",
        default="*",
        help="Test pull request only against images matching this pattern",
    )
    parser.add_argument(
        "--dry-run",
        help="Do not update pull request status or upload artifacts",
        default=False,
        action="store_true",
    )

    args = parser.parse_args()

    check_environment()

    if args.dry_run:
        token = ""
    else:
        with open(args.secrets + "/github-token") as tokenfile:
            token = tokenfile.read().strip()

    # default values for config
    config = {"repositories": [], "images": [], "name": "linux-system-roles-test"}

    with open(args.config + "/config.json") as configfile:
        config.update(json.load(configfile))
        images = config["images"]
        random.shuffle(images)
        repos = [r.split("/") for r in config.get("repositories", [])]
        random.shuffle(repos)

    setup_logging(config.get("logging", {}))

    gh = Session("https://api.github.com")
    gh.headers.update(
        {
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "linux-system-roles/test",
        }
    )

    if token:
        gh.headers.update({"Authorization": f"token {token}"})

    # Use CacheControl.
    # Requests on keep-alive connections fail when the remote has closed a
    # connection before we've noticed and sent another request. Thus,
    # always retry sending requests once.
    gh.mount(
        "https://",
        cachecontrol.CacheControlAdapter(max_retries=1, heuristic=DontCacheStatuses()),
    )

    printed_waiting = False

    # Random delay at startup to prevent triggering abuse detection on GitHub
    # when multiple instances are started in same time
    if not args.dry_run:
        time.sleep(random.randint(0, 60))

    for pull_request in args.pull_request:
        # supports to specify the PR with full URLs or just the path
        parsed_url = urllib.parse.urlparse(pull_request)
        owner, repo, pullnr = (
            parsed_url.path.strip("/").replace("/pull/", "/").split("/")
        )

        head = parsed_url.fragment
        pull = gh.get(f"repos/{owner}/{repo}/pulls/{pullnr}").json()

        if not head:
            head = pull["head"]["sha"]
        number = pull["number"]

        image_patterns = args.use_images.split(",")
        test_images = []
        for pattern in image_patterns:
            for image in images:
                if fnmatch.fnmatch(image["name"], pattern) or fnmatch.fnmatch(
                    image["source"], pattern
                ):
                    if image not in test_images:
                        test_images.append(image)

        for image in test_images:
            task = Task(owner, repo, number, head, image)
            handle_task(gh, args, config, task, args.dry_run)

    while not args.pull_request:
        try:
            task = choose_task(gh, repos, images, config)
            if not task:
                if not printed_waiting:
                    print(">>> No tasks. Waiting.")
                    printed_waiting = True
                time.sleep(600)
                continue

            handle_task(gh, args, config, task, args.dry_run)
        except requests.exceptions.HTTPError as err:
            if not handle_transient_httperrors(err):
                raise

        # At this point, we have (probably) printed other messages, so
        # reset printed_waiting
        printed_waiting = False


if __name__ == "__main__":
    sys.exit(main())
